{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "785e23fb7a10fac6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transformer Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ba02f4cf97389",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c03b7821046028",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508e547be8863b4c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from CustomLoss import CustomEmbeddingSliceLoss\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "#transformer\n",
    "NUM_HEADS = 6 # Dividers of 282: {1, 2, 3, 6, 47, 94, 141, 282}\n",
    "NUM_ENCODER_LAYERS = 4\n",
    "NUM_DECODER_LAYERS = 4\n",
    "DROPOUT=0.3\n",
    "\n",
    "# Methods\n",
    "loss_function = CustomEmbeddingSliceLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38275f8024e6a423",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf2a0f340d6c6fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "FEATURE_DIM = 282"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0797527f3ff4a27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Prepared Tensors from Disk\n",
    "Run file `prototype_dataset.ipynb` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9854fe06e9ade1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_sequence_input = torch.load('data/prototype_dataset/train_sequence_input.pt')\n",
    "train_sequence_output = torch.load('data/prototype_dataset/train_sequence_output.pt')\n",
    "test_sequence_input = torch.load('data/prototype_dataset/test_sequence_input.pt')\n",
    "test_sequence_output = torch.load('data/prototype_dataset/test_sequence_output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a0db6b3ad420b6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset_helper import warn_if_contains_NaN\n",
    "\n",
    "warn_if_contains_NaN(train_sequence_input)\n",
    "warn_if_contains_NaN(train_sequence_output)\n",
    "warn_if_contains_NaN(test_sequence_input)\n",
    "warn_if_contains_NaN(test_sequence_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29c9d0622c6c09e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([382, 95, 282])\n",
      "torch.Size([382, 95, 282])\n",
      "torch.Size([37, 95, 282])\n",
      "torch.Size([37, 95, 282])\n"
     ]
    }
   ],
   "source": [
    "print(train_sequence_input.size())\n",
    "print(train_sequence_output.size())\n",
    "print(test_sequence_input.size())\n",
    "print(test_sequence_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78852276c50093a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build Dataloader with Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_dataloader = DataLoader(TensorDataset(train_sequence_input.float(), train_sequence_output.float()),\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True)\n",
    "val_dataloader = DataLoader(TensorDataset(test_sequence_input.float(), test_sequence_output.float()),\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6d2274b93cd09",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ec1a0c79dbc2a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56536a90ec7c6586",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable anomaly detection\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db497beac7ffd3bb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf1dff2a6bc96474",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13102312 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from AnimationTransformer import AnimationTransformer\n",
    "\n",
    "model = AnimationTransformer(\n",
    "    dim_model=FEATURE_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    dropout_p=DROPOUT,\n",
    "    use_positional_encoder=True\n",
    ").to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_param = sum(p.numel() for p in model.parameters())\n",
    "print(f\"The model has {total_param} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e011ea2a897aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "504fb525334cb368",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 1: Time per Batch  1.10s | Total expected  0.20 min | Remaining  0.18 min \n",
      ">> 10: Time per Batch  0.75s | Total expected  0.14 min | Remaining  0.01 min \n",
      ">> Epoch time: 0.14 min\n",
      "Training loss: 28.4175\n",
      "Validation loss: 12.2519\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:380: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
     ]
    }
   ],
   "source": [
    "from AnimationTransformer import fit\n",
    "\n",
    "train_loss_list, validation_loss_list = fit(model,\n",
    "                                            optimizer,\n",
    "                                            loss_function,\n",
    "                                            train_dataloader,\n",
    "                                            val_dataloader,\n",
    "                                            epochs=1,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c920a75e5f59950",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n",
      ">> 1: Time per Batch  0.85s | Total expected  0.16 min | Remaining  0.14 min \n",
      ">> 10: Time per Batch  0.78s | Total expected  0.14 min | Remaining  0.01 min \n",
      ">> Epoch time: 0.14 min\n",
      "Training loss: 27.5234\n",
      "Validation loss: 12.2208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the number of additional epochs you want to train for\n",
    "additional_epochs = 1\n",
    "\n",
    "# Continue training the model for more epochs\n",
    "new_train_loss, new_validation_loss = fit(model,\n",
    "                                          optimizer,\n",
    "                                          loss_function,\n",
    "                                          train_dataloader,\n",
    "                                          val_dataloader,\n",
    "                                          epochs=additional_epochs,\n",
    "                                          device=device)\n",
    "\n",
    "# Extend the original loss lists with the new loss values\n",
    "train_loss_list.extend(new_train_loss)\n",
    "validation_loss_list.extend(new_validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f853ce1cf82a124",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      " 28,4175; 27,5234\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\\n\", \"; \".join([str(f\"{loss:.4f}\").replace('.', ',') for loss in train_loss_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d35cb0098fe9feb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      " 12,2519; 12,2208\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation\\n\", \"; \".join([str(f\"{loss:.4f}\").replace('.', ',') for loss in validation_loss_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1c6564934a7af",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training and Validation Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89297cea9ea4a9c6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Plot the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss_list, label='Training Loss')\n",
    "plt.plot(validation_loss_list, label='Validation Loss')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeac949784565",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3c8452421fbb912",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  -1.4797,   -1.4797, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000, -100.0000,\n",
       "        -100.0000, -100.0000, -100.0000, -100.0000, -100.0000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequences with long input\n",
    "test_sequence_input[30, :, 0] # 50, 10, 220, 222, 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b89e04b3d1425a49",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "            0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,  ...,   18.6053,   30.0000,\n",
      "            0.0000],\n",
      "        [   1.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "            0.0000],\n",
      "        ...,\n",
      "        [-100.0000, -100.0000, -100.0000,  ..., -100.0000, -100.0000,\n",
      "         -100.0000],\n",
      "        [-100.0000, -100.0000, -100.0000,  ..., -100.0000, -100.0000,\n",
      "         -100.0000],\n",
      "        [-100.0000, -100.0000, -100.0000,  ..., -100.0000, -100.0000,\n",
      "         -100.0000]])\n",
      "tensor([[   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
      "        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
      "        [   0.,    0.,    0.,  ...,    0.,    0.,    0.],\n",
      "        ...,\n",
      "        [-100., -100., -100.,  ..., -100., -100., -100.],\n",
      "        [-100., -100., -100.,  ..., -100., -100., -100.],\n",
      "        [-100., -100., -100.,  ..., -100., -100., -100.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# check sequence length / embedding\n",
    "print(test_sequence_output[20, :, -26:])\n",
    "print(test_sequence_input [20, :, -26:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3a328c640894ee5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a tensor of zeros with 270 elements\n",
    "sos_token = torch.zeros(282)\n",
    "# Set the value at the 256 index to 1\n",
    "sos_token[256] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1275cdf2526d3d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: Path 4 (13.373) got animation 1 (0.172%) with parameters [0.87, 1.55, -0.21, 1.11, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "3: Path 4 (13.358) got animation 1 (0.172%) with parameters [0.87, 1.56, -0.21, 1.12, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "4: Path 4 (13.358) got animation 1 (0.172%) with parameters [0.87, 1.56, -0.21, 1.13, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "5: Path 4 (13.358) got animation 1 (0.172%) with parameters [0.87, 1.56, -0.21, 1.13, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "6: Path 4 (13.358) got animation 1 (0.172%) with parameters [0.87, 1.56, -0.21, 1.13, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "7: Path 4 (13.358) got animation 1 (0.172%) with parameters [0.87, 1.57, -0.21, 1.13, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "8: Path 4 (13.358) got animation 1 (0.172%) with parameters [0.87, 1.57, -0.21, 1.13, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "9: Path 4 (13.358) got animation 1 (0.172%) with parameters [0.87, 1.57, -0.21, 1.13, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "10: Path 4 (13.359) got animation 1 (0.172%) with parameters [0.87, 1.57, -0.21, 1.13, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "11: Path 4 (13.359) got animation 1 (0.172%) with parameters [0.87, 1.57, -0.21, 1.13, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8400, -1.2773,  1.2629,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.8400, -1.2773,  1.2629,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        ...,\n",
      "        [ 0.8400, -1.2773,  1.2629,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.8400, -1.2773,  1.2629,  ..., -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.8400, -1.2773,  1.2629,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>) torch.Size([11, 282])\n"
     ]
    }
   ],
   "source": [
    "from AnimationTransformer import predict\n",
    "\n",
    "result = predict(model, test_sequence_input[10], sos_token=sos_token, device=device, max_length=10, eos_scaling=0.01)\n",
    "print(result, result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result = result.cpu()\n",
    "model_parameters = result[:,-26:].detach().numpy()\n",
    "print(model_parameters, model_parameters.shape)\n",
    "\n",
    "model_parameters = pd.DataFrame(model_parameters)\n",
    "model_parameters[\"model_output\"] = model_parameters.apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "# Apply the custom function to the \"model_output\" column\n",
    "model_parameters = model_parameters[['model_output']]\n",
    "\n",
    "model_parameters[\"animation_id\"] = range(0, len(model_parameters))\n",
    "print(model_parameters, model_parameters.shape)\n",
    "from src.postprocessing.postprocessing import animate_logo\n",
    "\n",
    "animate_logo(model_parameters, \"data/1_inserted_animation_id/logo_104.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785899ffda9c61f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac116ba60f873a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from AnimationTransformer import validation_loop, train_loop\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_EPOCHS = 5\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    num_encoder_layers = trial.suggest_categorical('num_encoder_layers', [2, 4, 6])\n",
    "    num_decoder_layers = trial.suggest_categorical('num_decoder_layers', [4, 6, 8])\n",
    "    # batch_size = trial.suggest_categorical('batch_size', [64])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [2, 3, 6])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.3)\n",
    "    use_positional_encoder = trial.suggest_categorical('pos_encoder_max_len', [True, False])\n",
    "    \n",
    "    print(f'Parameters selected')\n",
    "    print(f'num_encoder_layers; num_decoder_layers; learning_rate; num_heads; use_positional_encoder; dropout')\n",
    "    print(f'{num_encoder_layers}; {num_decoder_layers}; {learning_rate}; {num_heads}; {use_positional_encoder}; {dropout}'.replace('.', ','))\n",
    "    \n",
    "    # Instantiate the model with suggested hyperparameters\n",
    "    model = AnimationTransformer(\n",
    "        dim_model=FEATURE_DIM,\n",
    "        num_heads=num_heads,\n",
    "        num_encoder_layers=num_encoder_layers,\n",
    "        num_decoder_layers=num_decoder_layers,\n",
    "        dropout_p=dropout,\n",
    "        use_positional_encoder=use_positional_encoder\n",
    "    ).to(device)\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "    validation_loss = -1\n",
    "    # Training loop with early stopping, validation, etc.\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        print(f' =========== EPOCH {epoch} ===========')\n",
    "        \n",
    "        train_loss = train_loop(model, optimizer, loss_function, train_dataloader, device)\n",
    "        train_loss_list += [train_loss]\n",
    "\n",
    "        validation_loss = validation_loop(model, loss_function, val_dataloader, device)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}')\n",
    "        \n",
    "        # Report the validation loss to Optuna\n",
    "        trial.report(validation_loss, epoch)\n",
    "        \n",
    "        # Implement early stopping logic\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    print(f'Best validation loss: {validation_loss}')\n",
    "    print(f'num_encoder_layers; num_decoder_layers; learning_rate; num_heads; use_positional_encoder; dropout;')\n",
    "    print(f'{num_encoder_layers}; {num_decoder_layers}; {learning_rate:.8f}; {num_heads}; {use_positional_encoder}; {dropout:.8f}; Validation; '.replace('.', ','),\n",
    "          \"; \".join([str(f\"{loss:.4f}\").replace('.', ',') for loss in validation_loss_list]))\n",
    "    print(f' ; ; ; ; ; Train; ',\n",
    "          \"; \".join([str(f\"{loss:.4f}\").replace('.', ',') for loss in train_loss_list]))\n",
    "    \n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec6f9ac58486db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Used:\n",
    "- pick_and_animate_from_8     First Run\n",
    "- pick_and_animate_from_8_v3  First Main Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c795e29038042",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    study_name='pick_and_animate_from_8', # IMPORTANT: Chance Name when new Dataset\n",
    "    storage='sqlite:///animate_svg_optuna.db',\n",
    "    load_if_exists=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f41c46488c115",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#study = optuna.create_study(direction='minimize')\n",
    "my_study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68b3288e88b82c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Best trial:\")\n",
    "trial = my_study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4454b71fe58709",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "plot_optimization_history(my_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af745d741c9f91e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_param_importances(my_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9536888eb5d9ac",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_slice\n",
    "\n",
    "plot_slice(my_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b93522b8685dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_timeline\n",
    "\n",
    "plot_timeline(my_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed50888",
   "metadata": {},
   "source": [
    "## Reinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34e109bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m reward\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/models/reward_function_mode_state_dict_4.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     16\u001b[0m reward\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 17\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mreward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred, pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\Desktop\\team project\\Animate_SVG_v2\\transformer_for_reward_function.py:76\u001b[0m, in \u001b[0;36mRewardTransformer.forward\u001b[1;34m(self, src, tgt, tgt_mask, src_pad_mask, tgt_pad_mask, batch_first)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, tgt_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, src_pad_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tgt_pad_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Src size must be (batch_size, src sequence length)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Tgt size must be (batch_size, tgt sequence length)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m#src = self.embedding(src) * math.sqrt(self.dim_model)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;66;03m#tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoder(tgt)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m#print(src.shape, tgt.shape)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m#transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\Desktop\\team project\\Animate_SVG_v2\\transformer_for_reward_function.py:31\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[1;34m(self, token_embedding)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_embedding: torch\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Residual connection + pos encoding\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(token_embedding \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoding[:\u001b[43mtoken_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, :])\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "#REWARDFUNCTIONTEST\n",
    "\n",
    "from transformer_for_reward_function import RewardTransformer\n",
    "\n",
    "inputdata = torch.cat([train_sequence_output, test_sequence_output], dim=0).to(device)\n",
    "\n",
    "reward = RewardTransformer(\n",
    "            dim_model=282,\n",
    "            num_heads=6,\n",
    "            num_encoder_layers=8,\n",
    "            num_decoder_layers=8,\n",
    "            dropout_p=0.1,\n",
    "            num_tokens=inputdata[0:1,:,:].shape[1]).to(device)\n",
    "        \n",
    "reward.load_state_dict(torch.load(\"data/models/reward_function_mode_state_dict_4.pth\"))\n",
    "reward.eval()\n",
    "pred = reward(inputdata[1:2,:,:], inputdata[2:3,:,:])\n",
    "print(pred, pred.shape)\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "print(spaces.Box(low=0, high=1, shape=(100, 10), dtype=int).sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "073e9de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "            0.0000],\n",
      "        [   0.1792,   -0.5397,    0.2302,  ...,   58.3033,   46.0000,\n",
      "            0.0000],\n",
      "        [   1.1423,   -0.8956,    0.5215,  ...,   87.3264,   30.0000,\n",
      "            0.0000],\n",
      "        ...,\n",
      "        [-100.0000, -100.0000, -100.0000,  ..., -100.0000, -100.0000,\n",
      "         -100.0000],\n",
      "        [-100.0000, -100.0000, -100.0000,  ..., -100.0000, -100.0000,\n",
      "         -100.0000],\n",
      "        [-100.0000, -100.0000, -100.0000,  ..., -100.0000, -100.0000,\n",
      "         -100.0000]], device='cuda:0')\n",
      "1\n",
      "choose action for: tensor([[[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
      "         [ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
      "         [ 0.,  0.,  0.,  ..., -1., -1., -1.],\n",
      "         [ 0.,  0.,  0.,  ..., -1., -1., -1.]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>) torch.Size([1, 11, 282]) <class 'torch.Tensor'>\n",
      "torch.Size([1, 11, 282]) tensor(0.2540, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.0819, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.3219, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.2319, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(0.3756, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(0.0687, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.0656, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.0264, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.4523, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(0.0465, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(0.0776, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.0025, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.2605, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(0.1241, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "torch.Size([1, 11, 282]) tensor(-0.1742, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 172\u001b[0m\n\u001b[0;32m    170\u001b[0m inputdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([train_sequence_output, test_sequence_output], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m#print(inputdata, inputdata.shape)\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[43mreinforce\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 162\u001b[0m, in \u001b[0;36mreinforce\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    161\u001b[0m     current \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrainStep()\n\u001b[1;32m--> 162\u001b[0m     action, reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     state, reward, done \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action, reward)\n\u001b[0;32m    165\u001b[0m     agent\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m state\n",
      "Cell \u001b[1;32mIn[37], line 121\u001b[0m, in \u001b[0;36mAgent.chooseAction\u001b[1;34m(self, pred)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m#print(next[:, path, :], next[:, path, 256:266])\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28mnext\u001b[39m[:, path, \u001b[38;5;241m256\u001b[39m:\u001b[38;5;241m266\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(t)\n\u001b[1;32m--> 121\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m\u001b[38;5;241m.\u001b[39mshape, reward)\n\u001b[0;32m    123\u001b[0m possibleSteps\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mnext\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\Desktop\\team project\\Animate_SVG_v2\\transformer_for_reward_function.py:83\u001b[0m, in \u001b[0;36mRewardTransformer.forward\u001b[1;34m(self, src, tgt, tgt_mask, src_pad_mask, tgt_pad_mask, batch_first)\u001b[0m\n\u001b[0;32m     77\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoder(tgt)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m#print(src.shape, tgt.shape)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m#transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m transformer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_pad_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(transformer_out)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:206\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    204\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src, mask\u001b[38;5;241m=\u001b[39msrc_mask, src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask,\n\u001b[0;32m    205\u001b[0m                       is_causal\u001b[38;5;241m=\u001b[39msrc_is_causal)\n\u001b[1;32m--> 206\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:460\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    457\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 460\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:846\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    844\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x))\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 846\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    847\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[0;32m    848\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:855\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[0;32m    854\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 855\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\functional.py:5300\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[0;32m   5299\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m in_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 5300\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection_packed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5302\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m q_proj_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\functional.py:4827\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[1;34m(q, k, v, w, b)\u001b[0m\n\u001b[0;32m   4825\u001b[0m     \u001b[38;5;66;03m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[0;32m   4826\u001b[0m     proj \u001b[38;5;241m=\u001b[39m proj\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m3\u001b[39m, E))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m-> 4827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mproj\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, proj[\u001b[38;5;241m1\u001b[39m], proj[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   4828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;66;03m# encoder-decoder attention\u001b[39;00m\n\u001b[0;32m   4830\u001b[0m     w_q, w_kv \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39msplit([E, E \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\fx\\traceback.py:68\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\traceback.py:211\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 211\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\traceback.py:362\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    359\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    360\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals))\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 362\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from transformer_for_reward_function import RewardTransformer\n",
    "\n",
    "test = torch.tensor(0)\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self, startingState : torch.tensor):\n",
    "        super(CustomEnv, self).__init__()\n",
    "\n",
    "        # Define your action and observation space\n",
    "        self.action_space = spaces.Discrete(10)\n",
    "\n",
    "        # Example: 6-dimensional observation space represented by a Box\n",
    "        self.observation_space = startingState\n",
    "\n",
    "        self.rewardList = []\n",
    "\n",
    "        # Define any other environment parameters\n",
    "        self.max_steps = 1\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.reward = RewardTransformer(\n",
    "            dim_model=282,\n",
    "            num_heads=6,\n",
    "            num_encoder_layers=8,\n",
    "            num_decoder_layers=8,\n",
    "            dropout_p=0.1,\n",
    "            num_tokens=startingState.shape[1]).to(device)\n",
    "        \n",
    "        self.reward.load_state_dict(torch.load(\"data/models/reward_function_mode_state_dict_4.pth\"))\n",
    "        \n",
    "\n",
    "    def reset(self, startingState : torch.tensor):\n",
    "        # Reset the environment to its initial state\n",
    "        self.current_step = 0\n",
    "        self.observation_space = startingState\n",
    "        # Return the initial observation\n",
    "        return self.observation_space\n",
    "\n",
    "    def step(self, action, reward):\n",
    "        ##### STATE = APPLY ACTION TO OBSERVATION #####\n",
    "        state = action\n",
    "        \n",
    "        self.rewardList.append(reward)\n",
    "        \n",
    "        done = self.current_step >= self.max_steps\n",
    "        self.current_step += 1\n",
    "        return state, reward, done\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, input : torch.tensor, env : CustomEnv):\n",
    "\n",
    "        self.input = input\n",
    "        self.X = input[0].to(device)\n",
    "        print(\"X: \",self.X)\n",
    "        #self.y = self.randomAnimation(self.X).to(device)\n",
    "\n",
    "        self.currentLogo = 0\n",
    "\n",
    "        self.epsilon = 0.8\n",
    "\n",
    "        self.model = AnimationTransformer(\n",
    "            dim_model=FEATURE_DIM,\n",
    "            num_heads=NUM_HEADS,\n",
    "            num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "            num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "            dropout_p=DROPOUT,\n",
    "            use_positional_encoder=True\n",
    "        ).to(device)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        self.loss_fn = CustomEmbeddingSliceLoss()\n",
    "\n",
    "        self.total_loss = 0\n",
    "\n",
    "        self.env = env\n",
    "    \n",
    "    def randomAnimation(self, t : torch.tensor):\n",
    "        print(t.shape)\n",
    "        for i in range(t.shape[0]):\n",
    "            for j in range(t.shape[1]):\n",
    "                type = random.randint(0,10)\n",
    "                if t[i,j,256+type] != .100:\n",
    "                    t[i,j,256+type] = 1\n",
    "                    for d in range (266, 282):\n",
    "                        t[i,j,d] = random.random()\n",
    "        return t\n",
    "    \n",
    "    def trainStep(self):\n",
    "        \n",
    "        # Create a tensor of zeros with 270 elements\n",
    "        sos_token = torch.zeros(282)\n",
    "        # Set the value at the 256 index to 1\n",
    "        sos_token[256] = 1\n",
    "        \n",
    "        self.y = predict(self.model, self.X, sos_token=sos_token, device=device, max_length=10, eos_scaling=0.01, backpropagate=True, showResult=False)\n",
    "        \n",
    "        return torch.unsqueeze(self.y, dim=0)\n",
    "    \n",
    "    def chooseAction(self, pred : torch.tensor):\n",
    "        \n",
    "        print(\"choose action for:\", pred, pred.shape, type(pred))\n",
    "\n",
    "        possibleSteps = []\n",
    "        possibleStepsRewards = []\n",
    "\n",
    "        for path in range(pred.shape[1]):\n",
    "            for step in range(10):\n",
    "                next = pred \n",
    "                t = np.zeros(10, dtype=int)\n",
    "                t[step] = 1\n",
    "                #print(next[:, path, :], next[:, path, 256:266])\n",
    "                next[:, path, 256:266] = torch.tensor(t)\n",
    "                reward = self.env.reward(pred, next)[0,0,0]\n",
    "                print(next.shape, reward)\n",
    "                possibleSteps.append(next)\n",
    "                possibleStepsRewards.append(reward)\n",
    "\n",
    "        q = random.random()\n",
    "\n",
    "        if q > self.epsilon:\n",
    "            print(\"explore\")\n",
    "            idx = random.randint(0,len(possibleSteps)-1)\n",
    "        else:\n",
    "            print(\"exploit\")\n",
    "            idx = possibleStepsRewards.index(max(possibleStepsRewards))\n",
    "\n",
    "        action = possibleSteps[idx]\n",
    "        reward = possibleStepsRewards[idx]\n",
    "        return action, reward\n",
    "\n",
    "    def nextLogo(self):\n",
    "        self.X = self.input[self.currentLogo:self.currentLogo+1, :, :].to(device)\n",
    "        self.y = self.randomAnimation(self.X).to(device)\n",
    "\n",
    "        self.currentLogo = self.currentLogo+1\n",
    "    \n",
    "\n",
    "\n",
    "def reinforce():\n",
    "\n",
    "    env = CustomEnv(inputdata[0:1, :, :])\n",
    "    agent = Agent(inputdata, env)\n",
    "\n",
    "    totalReward = 0\n",
    "    for i in range(1, inputdata.shape[0]):\n",
    "        \n",
    "        print(i)\n",
    "        observation = env.reset(inputdata[i-1:i, :, :])\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "                \n",
    "            current = agent.trainStep()\n",
    "            action, reward = agent.chooseAction(current)\n",
    "\n",
    "            state, reward, done = env.step(action, reward)\n",
    "            agent.X = state\n",
    "            totalReward += reward\n",
    "\n",
    "        agent.nextLogo()\n",
    "\n",
    "inputdata = torch.cat([train_sequence_output, test_sequence_output], dim=0)\n",
    "#print(inputdata, inputdata.shape)\n",
    "reinforce()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eec347",
   "metadata": {},
   "source": [
    "## Animation Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c0bf1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
