{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset,random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir(\"./../.\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the output 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/surrogate_model/animation_label.pkl\", \"rb\") as f:\n",
    "    surrogate2 = pickle.load(f)\n",
    "\n",
    "#Filter with only good or very good ratings\n",
    "#filtered_output = surrogate[surrogate)][['label'].isin(['Good','Very Good'][\"file\",\"animation_id\",\"model_output\",\"label\"]]\n",
    "filtered_output2 = surrogate2[[\"file\",\"animation_id\",\"model_output\",\"label\"]]\n",
    "\n",
    "# dictionary for mapping\n",
    "\n",
    "mapping_dict = {\"Very Good\": 6, \"Good\": 5, \"Bad\": 4,\"Okay\":3, \"Very Bad\": 2, \"no_rating\": 1}\n",
    "\n",
    "# Create another column changing the label into ints\n",
    "\n",
    "filtered_output2['label2'] = filtered_output2['label'].replace(mapping_dict)\n",
    "\n",
    "\n",
    "# get the names of unique logos by splitting with animation number\n",
    "logos = filtered_output2[\"file\"].str.split(\"_animation\").str[0].unique()\n",
    "\n",
    "#print(logos)\n",
    "\n",
    "# create a data frame for the collected best animations\n",
    "bestoutput2 = pd.DataFrame()\n",
    "\n",
    "# go through each logo to find the best animation\n",
    "for logo in logos:\n",
    "\n",
    "    # make a data frame that contains all the animations of one logo\n",
    "    temp = filtered_output2[filtered_output2[\"file\"].str.contains(logo)]\n",
    "\n",
    "    #display(temp)\n",
    "\n",
    "    # create a sum \n",
    "    mean_by_label = temp.groupby('file')['label2'].mean().reset_index()\n",
    "\n",
    "    #print(mean_by_label)\n",
    "\n",
    "    bestlogo = mean_by_label.loc[mean_by_label['label2'].idxmax()]\n",
    "\n",
    "    #print(bestlogo)\n",
    "\n",
    "    # get all the animated paths with the best animation of the logo\n",
    "    best_animations2 = temp[temp[\"file\"]==bestlogo[\"file\"]]\n",
    "\n",
    "    # add to the file\n",
    "    bestoutput2 = pd.concat([bestoutput2,best_animations2],axis=0, ignore_index=True)\n",
    "bestoutput2 = bestoutput2.sort_values(by=['file','animation_id'])\n",
    "display(bestoutput2)\n",
    "\n",
    "filenames = bestoutput2[\"file\"].unique()\n",
    "list = []\n",
    "for name in filenames:\n",
    "    seq = bestoutput2[bestoutput2[\"file\"]==name]\n",
    "    seq = seq[\"model_output\"]\n",
    "    seq = pd.DataFrame(bestoutput2[\"model_output\"].to_list(), columns=[\"a1\",\"a2\",\"a3\",\"a4\",\"a5\",\"a6\",\"a7\",\"a8\",\"a9\",\"a10\",\"a11\",\"a12\"])\n",
    "    \n",
    "    seq = pd.concat([pd.DataFrame(10, index=seq.index, columns=range(0, 256)), seq], axis=1, ignore_index=True)\n",
    "    \n",
    "    if len(seq) > 4:\n",
    "        seq = seq[:4]\n",
    "\n",
    "    sos = pd.DataFrame([[30]*268])\n",
    "    \n",
    "    eos = pd.DataFrame([[50]*268])\n",
    "\n",
    "    seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "    \n",
    "    while len(seq) < 6:\n",
    "           seq = pd.concat([seq, pd.DataFrame([[-100]*268])], ignore_index=True)\n",
    "           \n",
    "    #seq = seq.apply(lambda x: np.array(x).astype(np.float32))\n",
    "    #tokens = []\n",
    "    #for l in seq:\n",
    "    #    tokens.append(torch.tensor(l))\n",
    "\n",
    "    list.append(torch.tensor(seq.values))\n",
    "outTensor2 = torch.stack(list)\n",
    "\n",
    "outTensor2 = outTensor2.to(device)\n",
    "print(outTensor2.max())\n",
    "print(outTensor2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input tensor with the diltered output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data\\embeddings\\path_embedding.pkl\", \"rb\") as f:\n",
    "    inp = pickle.load(f)\n",
    "\n",
    "bestoutput2[\"filename\"] = bestoutput2[\"file\"].str.split(\"_animation\").str[0]\n",
    "\n",
    "display(bestoutput2)\n",
    "\n",
    "inp['animation_id'] = inp['animation_id'].astype(int)\n",
    "\n",
    "#names = bestoutput2[\"file\"].str.replace(\"_animation_0\", \"\")\n",
    "\n",
    "#input = inp[(inp[\"filename\"].isin(bestoutput2[\"filename\"])) & (inp[\"animation_id\"].isin(bestoutput2[\"animation_id\"]))]\n",
    "input = pd.merge(bestoutput2, inp, on=['filename', 'animation_id'],how='inner')\n",
    "input = input.drop(['model_output', 'label','label2','file'], axis=1)\n",
    "\n",
    "input = input.sort_values(by=['filename','animation_id'])\n",
    "display(input)\n",
    "filenames = input[\"filename\"].unique()\n",
    "#print(filenames)\n",
    "list = []\n",
    "for name in filenames:\n",
    "    #print(name)\n",
    "    seq = input[input[\"filename\"]==name].loc[:, ~inp.columns.isin([\"filename\",\"animation_id\"])][:4]\n",
    "    #print(seq)\n",
    "    seq = pd.concat([seq, pd.DataFrame(10, index=seq.index, columns=range(256, 268))], axis=1, ignore_index=True)\n",
    "\n",
    "    while len(seq) < 4:\n",
    "        seq = pd.concat([seq, pd.DataFrame([[10]*268])], ignore_index=True)\n",
    "\n",
    "    sos = pd.DataFrame([[30]*268])\n",
    "\n",
    "    \n",
    "    eos = pd.DataFrame([[50]*268])\n",
    "\n",
    "    seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "\n",
    "    list.append(torch.tensor(seq.values))\n",
    "    #print(list)\n",
    "inpTensor2 = torch.stack(list)\n",
    "inpTensor2 = inpTensor2.to(device)\n",
    "print(inpTensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inpTensor2[:1,:,:]\n",
    "test = test.to(torch.float32)\n",
    "test = test.to(device)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        batch_size = Q.size()[0]\n",
    "        seq_length = Q.size()[1]\n",
    "        #mask = mask.view(batch_size, 1, 1, seq_length).expand(-1, self.num_heads, -1, -1)\n",
    "        mask = mask.unsqueeze(1).unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "        mask = mask.permute(0, 1, 3, 2)\n",
    "        #print(\"mask shape:\",mask.shape)\n",
    "        #print(attn_scores.shape)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == True, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        #print(\"Q:\",Q.shape)\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "\n",
    "        #d_ff: dimension of hidden layer\n",
    "\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding2(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding2, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFromscratch(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(TransformerFromscratch, self).__init__()\n",
    "        #self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        #self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding2(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 10).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 10).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        print(\"seq_length\",seq_length)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        print(\"nopeak:\",nopeak_mask.shape)\n",
    "        print(\"tgt_mask:\",tgt_mask.shape)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def create_pad_mask(self, input : torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "\n",
    "        t = []\n",
    "        for seq in input:\n",
    "            list =[]\n",
    "            for token in seq:\n",
    "                b = False\n",
    "                for value in token:\n",
    "                    if value == pad_token:\n",
    "                        b = True\n",
    "                list.append(b)\n",
    "            t.append(list)\n",
    "        return torch.tensor(t)\n",
    "\n",
    "        #return (input == pad_token)\n",
    "    \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a square matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        #src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_mask = self.create_pad_mask(src,pad_token=10).to(device)\n",
    "        tgt_mask = self.create_pad_mask(tgt,pad_token=10).to(device)\n",
    "        src_embedded = self.dropout(self.positional_encoding(src))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(tgt))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = TransformerFromscratch(tgt_vocab_size=268, d_model=268, num_heads=4, num_layers=8, d_ff=2048, dropout=0.1,max_seq_length=10)\n",
    "model2.to(device)\n",
    "opt = torch.optim.SGD(model2.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[0], batch[1]\n",
    "        #X, y = torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        #sequence_length = y.size(1)\n",
    "        #tgt_mask = model.get_tgt_mask(sequence_length)\n",
    "        \n",
    "        #print(y.view(y.size(0), -1).shape)\n",
    "        #pad_mask = model.create_pad_mask(y, pad_token=-100)\n",
    "        #print(pad_mask)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        \n",
    "        #print(torch.isnan(X).any())\n",
    "        #print(torch.isnan(y).any())\n",
    "\n",
    "        #pred = model(X, y, tgt_mask, pad_mask)\n",
    "\n",
    "        pred = model2(X,y)\n",
    "\n",
    "        #print(\"pred shape:\",pred.shape)\n",
    "        #print(\"y shape:\",y.shape)\n",
    "\n",
    "        y_flattened = y.contiguous().view(-1)\n",
    "\n",
    "        #print(y_flattened)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        #pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #print(\"prediction:\",pred)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # tryign to print the gradient\n",
    "        #for p in model.parameters():\n",
    "        #    print(p.grad.norm())\n",
    "\n",
    "        #for name, param in model2.named_parameters():\n",
    "        #    if 'weight' in name:\n",
    "         #       print(name)\n",
    "         #       print(param.data.cpu().numpy().shape)\n",
    "         #       print('gradient is \\t', param.grad, '\\trequires grad: ', param.requires_grad)\n",
    "\n",
    "        # gradient clipping to avoid the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_value_(model2.parameters(), 10.)\n",
    "\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train = TensorDataset(inpTensor2.float(), outTensor2.float())\n",
    "batch_size = 200 # Set your desired batch size\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)  # For input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model2, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        #validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        #validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        #print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "train_loss_list, validation_loss_list = fit(model2, opt, loss_fn, train_dataloader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=6, SOS_token=[[30] * 268], EOS_token=[[50] * 268]):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.float32)\n",
    "\n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        #tgt_mask = model.get_tgt_mask(y_input.size(1))\n",
    "        \n",
    "        print(input_sequence.shape, y_input.shape)\n",
    "        pred = model(input_sequence, y_input)\n",
    "        print(pred.shape)\n",
    "        print(pred)\n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]])\n",
    "        \n",
    "        next_item = pred[:,:1,:]\n",
    "        print(next_item.shape)\n",
    "\n",
    "        #print(y_input, next_item)\n",
    "        # Concatenate previous input with predicted best word\n",
    "        #print(y_input.shape, next_item.shape)\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "        print(next_item[0][0][0])\n",
    "        # Stop if model predicts end of sentence\n",
    "        print(next_item.view(-1).shape)\n",
    "        if next_item[0][0][0] == EOS_token:\n",
    "        #if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    "\n",
    "svg_animations = predict(model2, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('AnimateSVG/src')\n",
    "from AnimateSVG.src.pipeline import *\n",
    "\n",
    "for i, row in svg_animations.iterrows():\n",
    "            try:\n",
    "                self._insert_animation(row['animation_id'], row['animation_vector'], filename_suffix=row['model'])\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {row['filename']}\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsvg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
