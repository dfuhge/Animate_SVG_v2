{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset,random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir(\"./../.\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1591245a580>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the output 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\okan2\\AppData\\Local\\Temp\\ipykernel_11640\\445249230.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_output2['label2'] = filtered_output2['label'].replace(mapping_dict)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>animation_id</th>\n",
       "      <th>model_output</th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, -1.0, -1.0, -1.0, 0.4205715...</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>21</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.4...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>22</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>22</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file  animation_id  \\\n",
       "436   logo_0_animation_0             0   \n",
       "439   logo_0_animation_0             1   \n",
       "438   logo_0_animation_0             3   \n",
       "437   logo_0_animation_0             4   \n",
       "435   logo_0_animation_0             5   \n",
       "..                   ...           ...   \n",
       "892  logo_99_animation_0            21   \n",
       "811  logo_99_animation_0            22   \n",
       "891  logo_99_animation_0            22   \n",
       "810  logo_99_animation_0            23   \n",
       "890  logo_99_animation_0            23   \n",
       "\n",
       "                                          model_output      label  label2  \n",
       "436  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...       Good     5.0  \n",
       "439  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....       Good     5.0  \n",
       "438  [0, 0, 0, 0, 0, 1, -1.0, -1.0, -1.0, -1.0, -1....       Good     5.0  \n",
       "437  [0, 0, 1, 0, 0, 0, -1.0, -1.0, -1.0, 0.4205715...       Good     5.0  \n",
       "435  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....       Good     5.0  \n",
       "..                                                 ...        ...     ...  \n",
       "892  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.4...  Very Good     6.0  \n",
       "811  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...  Very Good     6.0  \n",
       "891  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...  Very Good     6.0  \n",
       "810  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....  Very Good     6.0  \n",
       "890  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....  Very Good     6.0  \n",
       "\n",
       "[907 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(50., dtype=torch.float64)\n",
      "torch.Size([359, 6, 268])\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/surrogate_model/animation_label.pkl\", \"rb\") as f:\n",
    "    surrogate2 = pickle.load(f)\n",
    "\n",
    "#Filter with only good or very good ratings\n",
    "#filtered_output = surrogate[surrogate)][['label'].isin(['Good','Very Good'][\"file\",\"animation_id\",\"model_output\",\"label\"]]\n",
    "filtered_output2 = surrogate2[[\"file\",\"animation_id\",\"model_output\",\"label\"]]\n",
    "\n",
    "# dictionary for mapping\n",
    "\n",
    "mapping_dict = {\"Very Good\": 6, \"Good\": 5, \"Bad\": 4,\"Okay\":3, \"Very Bad\": 2, \"no_rating\": 1}\n",
    "\n",
    "# Create another column changing the label into ints\n",
    "\n",
    "filtered_output2['label2'] = filtered_output2['label'].replace(mapping_dict)\n",
    "\n",
    "\n",
    "# get the names of unique logos by splitting with animation number\n",
    "logos = filtered_output2[\"file\"].str.split(\"_animation\").str[0].unique()\n",
    "\n",
    "#print(logos)\n",
    "\n",
    "# create a data frame for the collected best animations\n",
    "bestoutput2 = pd.DataFrame()\n",
    "\n",
    "# go through each logo to find the best animation\n",
    "for logo in logos:\n",
    "\n",
    "    # make a data frame that contains all the animations of one logo\n",
    "    temp = filtered_output2[filtered_output2[\"file\"].str.contains(logo)]\n",
    "\n",
    "    #display(temp)\n",
    "\n",
    "    # create a sum \n",
    "    mean_by_label = temp.groupby('file')['label2'].mean().reset_index()\n",
    "\n",
    "    #print(mean_by_label)\n",
    "\n",
    "    bestlogo = mean_by_label.loc[mean_by_label['label2'].idxmax()]\n",
    "\n",
    "    #print(bestlogo)\n",
    "\n",
    "    # get all the animated paths with the best animation of the logo\n",
    "    best_animations2 = temp[temp[\"file\"]==bestlogo[\"file\"]]\n",
    "\n",
    "    # add to the file\n",
    "    bestoutput2 = pd.concat([bestoutput2,best_animations2],axis=0, ignore_index=True)\n",
    "bestoutput2 = bestoutput2.sort_values(by=['file','animation_id'])\n",
    "display(bestoutput2)\n",
    "\n",
    "filenames = bestoutput2[\"file\"].unique()\n",
    "list = []\n",
    "for name in filenames:\n",
    "    seq = bestoutput2[bestoutput2[\"file\"]==name]\n",
    "    seq = seq[\"model_output\"]\n",
    "    seq = pd.DataFrame(bestoutput2[\"model_output\"].to_list(), columns=[\"a1\",\"a2\",\"a3\",\"a4\",\"a5\",\"a6\",\"a7\",\"a8\",\"a9\",\"a10\",\"a11\",\"a12\"])\n",
    "    \n",
    "    seq = pd.concat([pd.DataFrame(10, index=seq.index, columns=range(0, 256)), seq], axis=1, ignore_index=True)\n",
    "    \n",
    "    if len(seq) > 4:\n",
    "        seq = seq[:4]\n",
    "\n",
    "    sos = pd.DataFrame([[30]*268])\n",
    "    \n",
    "    eos = pd.DataFrame([[50]*268])\n",
    "\n",
    "    seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "    \n",
    "    while len(seq) < 6:\n",
    "           seq = pd.concat([seq, pd.DataFrame([[-100]*268])], ignore_index=True)\n",
    "           \n",
    "    #seq = seq.apply(lambda x: np.array(x).astype(np.float32))\n",
    "    #tokens = []\n",
    "    #for l in seq:\n",
    "    #    tokens.append(torch.tensor(l))\n",
    "\n",
    "    list.append(torch.tensor(seq.values))\n",
    "outTensor2 = torch.stack(list)\n",
    "\n",
    "outTensor2 = outTensor2.to(device)\n",
    "print(outTensor2.max())\n",
    "print(outTensor2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input tensor with the diltered output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>animation_id</th>\n",
       "      <th>model_output</th>\n",
       "      <th>label</th>\n",
       "      <th>label2</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>logo_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>logo_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>logo_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, -1.0, -1.0, -1.0, 0.4205715...</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>logo_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>logo_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>21</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.4...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>logo_99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>22</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>logo_99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>22</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>logo_99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>logo_99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6.0</td>\n",
       "      <td>logo_99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file  animation_id  \\\n",
       "436   logo_0_animation_0             0   \n",
       "439   logo_0_animation_0             1   \n",
       "438   logo_0_animation_0             3   \n",
       "437   logo_0_animation_0             4   \n",
       "435   logo_0_animation_0             5   \n",
       "..                   ...           ...   \n",
       "892  logo_99_animation_0            21   \n",
       "811  logo_99_animation_0            22   \n",
       "891  logo_99_animation_0            22   \n",
       "810  logo_99_animation_0            23   \n",
       "890  logo_99_animation_0            23   \n",
       "\n",
       "                                          model_output      label  label2  \\\n",
       "436  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...       Good     5.0   \n",
       "439  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....       Good     5.0   \n",
       "438  [0, 0, 0, 0, 0, 1, -1.0, -1.0, -1.0, -1.0, -1....       Good     5.0   \n",
       "437  [0, 0, 1, 0, 0, 0, -1.0, -1.0, -1.0, 0.4205715...       Good     5.0   \n",
       "435  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....       Good     5.0   \n",
       "..                                                 ...        ...     ...   \n",
       "892  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.4...  Very Good     6.0   \n",
       "811  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...  Very Good     6.0   \n",
       "891  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...  Very Good     6.0   \n",
       "810  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....  Very Good     6.0   \n",
       "890  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....  Very Good     6.0   \n",
       "\n",
       "    filename  \n",
       "436   logo_0  \n",
       "439   logo_0  \n",
       "438   logo_0  \n",
       "437   logo_0  \n",
       "435   logo_0  \n",
       "..       ...  \n",
       "892  logo_99  \n",
       "811  logo_99  \n",
       "891  logo_99  \n",
       "810  logo_99  \n",
       "890  logo_99  \n",
       "\n",
       "[907 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animation_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>logo_0</td>\n",
       "      <td>0.763518</td>\n",
       "      <td>-0.982797</td>\n",
       "      <td>-0.446681</td>\n",
       "      <td>1.089468</td>\n",
       "      <td>-0.070563</td>\n",
       "      <td>0.710206</td>\n",
       "      <td>-0.491675</td>\n",
       "      <td>-1.631172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339061</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.195161</td>\n",
       "      <td>-0.046488</td>\n",
       "      <td>-0.492103</td>\n",
       "      <td>-0.605836</td>\n",
       "      <td>-1.282879</td>\n",
       "      <td>0.613195</td>\n",
       "      <td>0.297194</td>\n",
       "      <td>-0.172312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>logo_0</td>\n",
       "      <td>0.851117</td>\n",
       "      <td>-1.775123</td>\n",
       "      <td>0.649689</td>\n",
       "      <td>-0.688600</td>\n",
       "      <td>0.216071</td>\n",
       "      <td>0.135211</td>\n",
       "      <td>-1.748761</td>\n",
       "      <td>-1.347670</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.616431</td>\n",
       "      <td>-1.572003</td>\n",
       "      <td>0.242460</td>\n",
       "      <td>0.430259</td>\n",
       "      <td>0.079752</td>\n",
       "      <td>-1.039526</td>\n",
       "      <td>-0.696104</td>\n",
       "      <td>0.090277</td>\n",
       "      <td>-0.228757</td>\n",
       "      <td>0.144372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>logo_0</td>\n",
       "      <td>0.291136</td>\n",
       "      <td>-0.928242</td>\n",
       "      <td>0.265542</td>\n",
       "      <td>-0.261439</td>\n",
       "      <td>-0.386160</td>\n",
       "      <td>1.256256</td>\n",
       "      <td>-0.414706</td>\n",
       "      <td>-1.206105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446020</td>\n",
       "      <td>-1.369758</td>\n",
       "      <td>0.356421</td>\n",
       "      <td>1.456656</td>\n",
       "      <td>0.468766</td>\n",
       "      <td>-1.077724</td>\n",
       "      <td>-0.548627</td>\n",
       "      <td>-0.300660</td>\n",
       "      <td>0.632805</td>\n",
       "      <td>-0.136473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>logo_0</td>\n",
       "      <td>0.504446</td>\n",
       "      <td>-0.543099</td>\n",
       "      <td>0.915062</td>\n",
       "      <td>1.293575</td>\n",
       "      <td>-0.849605</td>\n",
       "      <td>1.120387</td>\n",
       "      <td>-0.637641</td>\n",
       "      <td>-1.337280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162753</td>\n",
       "      <td>0.206993</td>\n",
       "      <td>-0.201259</td>\n",
       "      <td>-1.087391</td>\n",
       "      <td>-0.597388</td>\n",
       "      <td>-0.992079</td>\n",
       "      <td>-0.851486</td>\n",
       "      <td>-0.225463</td>\n",
       "      <td>-0.549269</td>\n",
       "      <td>0.088637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>logo_0</td>\n",
       "      <td>-0.641569</td>\n",
       "      <td>-0.657125</td>\n",
       "      <td>-0.105109</td>\n",
       "      <td>-0.031630</td>\n",
       "      <td>-0.572032</td>\n",
       "      <td>0.912017</td>\n",
       "      <td>-0.569627</td>\n",
       "      <td>-1.573482</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.079276</td>\n",
       "      <td>0.285482</td>\n",
       "      <td>1.532865</td>\n",
       "      <td>-0.375210</td>\n",
       "      <td>-0.249130</td>\n",
       "      <td>-0.551393</td>\n",
       "      <td>-1.024246</td>\n",
       "      <td>0.623726</td>\n",
       "      <td>-1.073305</td>\n",
       "      <td>0.166613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>21</td>\n",
       "      <td>logo_99</td>\n",
       "      <td>0.729620</td>\n",
       "      <td>-1.035586</td>\n",
       "      <td>0.390709</td>\n",
       "      <td>1.910684</td>\n",
       "      <td>-0.489203</td>\n",
       "      <td>2.197111</td>\n",
       "      <td>0.424709</td>\n",
       "      <td>-0.389426</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130544</td>\n",
       "      <td>-1.892641</td>\n",
       "      <td>-0.690288</td>\n",
       "      <td>1.371062</td>\n",
       "      <td>0.718980</td>\n",
       "      <td>-0.827454</td>\n",
       "      <td>-0.557172</td>\n",
       "      <td>0.235162</td>\n",
       "      <td>0.523978</td>\n",
       "      <td>-0.592003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>22</td>\n",
       "      <td>logo_99</td>\n",
       "      <td>1.147938</td>\n",
       "      <td>-0.799806</td>\n",
       "      <td>0.664411</td>\n",
       "      <td>1.610482</td>\n",
       "      <td>-0.612273</td>\n",
       "      <td>1.194499</td>\n",
       "      <td>0.076532</td>\n",
       "      <td>-0.662109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415811</td>\n",
       "      <td>-1.569571</td>\n",
       "      <td>-0.424470</td>\n",
       "      <td>0.518902</td>\n",
       "      <td>0.160598</td>\n",
       "      <td>-1.283986</td>\n",
       "      <td>-0.463894</td>\n",
       "      <td>0.319661</td>\n",
       "      <td>0.785325</td>\n",
       "      <td>-0.294870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>22</td>\n",
       "      <td>logo_99</td>\n",
       "      <td>1.147938</td>\n",
       "      <td>-0.799806</td>\n",
       "      <td>0.664411</td>\n",
       "      <td>1.610482</td>\n",
       "      <td>-0.612273</td>\n",
       "      <td>1.194499</td>\n",
       "      <td>0.076532</td>\n",
       "      <td>-0.662109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415811</td>\n",
       "      <td>-1.569571</td>\n",
       "      <td>-0.424470</td>\n",
       "      <td>0.518902</td>\n",
       "      <td>0.160598</td>\n",
       "      <td>-1.283986</td>\n",
       "      <td>-0.463894</td>\n",
       "      <td>0.319661</td>\n",
       "      <td>0.785325</td>\n",
       "      <td>-0.294870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>23</td>\n",
       "      <td>logo_99</td>\n",
       "      <td>1.645128</td>\n",
       "      <td>-1.911424</td>\n",
       "      <td>0.806118</td>\n",
       "      <td>0.850210</td>\n",
       "      <td>0.554424</td>\n",
       "      <td>1.578238</td>\n",
       "      <td>-0.722733</td>\n",
       "      <td>-0.704658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883759</td>\n",
       "      <td>-2.000007</td>\n",
       "      <td>-0.517095</td>\n",
       "      <td>2.213172</td>\n",
       "      <td>0.733567</td>\n",
       "      <td>-0.577074</td>\n",
       "      <td>-0.710312</td>\n",
       "      <td>-0.591997</td>\n",
       "      <td>0.836300</td>\n",
       "      <td>-0.720247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>23</td>\n",
       "      <td>logo_99</td>\n",
       "      <td>1.645128</td>\n",
       "      <td>-1.911424</td>\n",
       "      <td>0.806118</td>\n",
       "      <td>0.850210</td>\n",
       "      <td>0.554424</td>\n",
       "      <td>1.578238</td>\n",
       "      <td>-0.722733</td>\n",
       "      <td>-0.704658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883759</td>\n",
       "      <td>-2.000007</td>\n",
       "      <td>-0.517095</td>\n",
       "      <td>2.213172</td>\n",
       "      <td>0.733567</td>\n",
       "      <td>-0.577074</td>\n",
       "      <td>-0.710312</td>\n",
       "      <td>-0.591997</td>\n",
       "      <td>0.836300</td>\n",
       "      <td>-0.720247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows Ã— 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     animation_id filename         0         1         2         3         4  \\\n",
       "0               0   logo_0  0.763518 -0.982797 -0.446681  1.089468 -0.070563   \n",
       "1               1   logo_0  0.851117 -1.775123  0.649689 -0.688600  0.216071   \n",
       "2               3   logo_0  0.291136 -0.928242  0.265542 -0.261439 -0.386160   \n",
       "3               4   logo_0  0.504446 -0.543099  0.915062  1.293575 -0.849605   \n",
       "4               5   logo_0 -0.641569 -0.657125 -0.105109 -0.031630 -0.572032   \n",
       "..            ...      ...       ...       ...       ...       ...       ...   \n",
       "902            21  logo_99  0.729620 -1.035586  0.390709  1.910684 -0.489203   \n",
       "903            22  logo_99  1.147938 -0.799806  0.664411  1.610482 -0.612273   \n",
       "904            22  logo_99  1.147938 -0.799806  0.664411  1.610482 -0.612273   \n",
       "905            23  logo_99  1.645128 -1.911424  0.806118  0.850210  0.554424   \n",
       "906            23  logo_99  1.645128 -1.911424  0.806118  0.850210  0.554424   \n",
       "\n",
       "            5         6         7  ...       246       247       248  \\\n",
       "0    0.710206 -0.491675 -1.631172  ...  0.339061  0.022934  0.195161   \n",
       "1    0.135211 -1.748761 -1.347670  ... -1.616431 -1.572003  0.242460   \n",
       "2    1.256256 -0.414706 -1.206105  ... -0.446020 -1.369758  0.356421   \n",
       "3    1.120387 -0.637641 -1.337280  ...  0.162753  0.206993 -0.201259   \n",
       "4    0.912017 -0.569627 -1.573482  ... -1.079276  0.285482  1.532865   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "902  2.197111  0.424709 -0.389426  ...  1.130544 -1.892641 -0.690288   \n",
       "903  1.194499  0.076532 -0.662109  ...  0.415811 -1.569571 -0.424470   \n",
       "904  1.194499  0.076532 -0.662109  ...  0.415811 -1.569571 -0.424470   \n",
       "905  1.578238 -0.722733 -0.704658  ...  0.883759 -2.000007 -0.517095   \n",
       "906  1.578238 -0.722733 -0.704658  ...  0.883759 -2.000007 -0.517095   \n",
       "\n",
       "          249       250       251       252       253       254       255  \n",
       "0   -0.046488 -0.492103 -0.605836 -1.282879  0.613195  0.297194 -0.172312  \n",
       "1    0.430259  0.079752 -1.039526 -0.696104  0.090277 -0.228757  0.144372  \n",
       "2    1.456656  0.468766 -1.077724 -0.548627 -0.300660  0.632805 -0.136473  \n",
       "3   -1.087391 -0.597388 -0.992079 -0.851486 -0.225463 -0.549269  0.088637  \n",
       "4   -0.375210 -0.249130 -0.551393 -1.024246  0.623726 -1.073305  0.166613  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "902  1.371062  0.718980 -0.827454 -0.557172  0.235162  0.523978 -0.592003  \n",
       "903  0.518902  0.160598 -1.283986 -0.463894  0.319661  0.785325 -0.294870  \n",
       "904  0.518902  0.160598 -1.283986 -0.463894  0.319661  0.785325 -0.294870  \n",
       "905  2.213172  0.733567 -0.577074 -0.710312 -0.591997  0.836300 -0.720247  \n",
       "906  2.213172  0.733567 -0.577074 -0.710312 -0.591997  0.836300 -0.720247  \n",
       "\n",
       "[907 rows x 258 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.7635, -0.9828, -0.4467,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 0.8511, -1.7751,  0.6497,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 0.2911, -0.9282,  0.2655,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 0.5044, -0.5431,  0.9151,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.9591,  0.4688,  0.3871,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [10.0000, 10.0000, 10.0000,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [10.0000, 10.0000, 10.0000,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [10.0000, 10.0000, 10.0000,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 1.1227, -1.4871,  1.3754,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 0.5335, -1.3773,  0.6205,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 0.6370, -1.5410,  0.7299,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 0.5269, -1.5036,  0.6643,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.6488, -0.1748,  0.7375,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 1.4133, -0.7414,  0.1002,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [10.0000, 10.0000, 10.0000,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [10.0000, 10.0000, 10.0000,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.4118, -0.7754,  0.8243,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 1.2032, -1.1528,  0.8183,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 0.8463, -1.2633,  0.5078,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [10.0000, 10.0000, 10.0000,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.7296, -1.0356,  0.3907,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 0.7296, -1.0356,  0.3907,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 1.1479, -0.7998,  0.6644,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [ 1.1479, -0.7998,  0.6644,  ..., 10.0000, 10.0000, 10.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([359, 6, 268])\n"
     ]
    }
   ],
   "source": [
    "with open(\"data\\embeddings\\path_embedding.pkl\", \"rb\") as f:\n",
    "    inp = pickle.load(f)\n",
    "\n",
    "bestoutput2[\"filename\"] = bestoutput2[\"file\"].str.split(\"_animation\").str[0]\n",
    "\n",
    "display(bestoutput2)\n",
    "\n",
    "inp['animation_id'] = inp['animation_id'].astype(int)\n",
    "\n",
    "#names = bestoutput2[\"file\"].str.replace(\"_animation_0\", \"\")\n",
    "\n",
    "#input = inp[(inp[\"filename\"].isin(bestoutput2[\"filename\"])) & (inp[\"animation_id\"].isin(bestoutput2[\"animation_id\"]))]\n",
    "input = pd.merge(bestoutput2, inp, on=['filename', 'animation_id'],how='inner')\n",
    "input = input.drop(['model_output', 'label','label2','file'], axis=1)\n",
    "\n",
    "input = input.sort_values(by=['filename','animation_id'])\n",
    "display(input)\n",
    "filenames = input[\"filename\"].unique()\n",
    "#print(filenames)\n",
    "list = []\n",
    "for name in filenames:\n",
    "    #print(name)\n",
    "    seq = input[input[\"filename\"]==name].loc[:, ~inp.columns.isin([\"filename\",\"animation_id\"])][:4]\n",
    "    #print(seq)\n",
    "    seq = pd.concat([seq, pd.DataFrame(10, index=seq.index, columns=range(256, 268))], axis=1, ignore_index=True)\n",
    "\n",
    "    while len(seq) < 4:\n",
    "        seq = pd.concat([seq, pd.DataFrame([[10]*268])], ignore_index=True)\n",
    "\n",
    "    sos = pd.DataFrame([[30]*268])\n",
    "\n",
    "    \n",
    "    eos = pd.DataFrame([[50]*268])\n",
    "\n",
    "    seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "\n",
    "    list.append(torch.tensor(seq.values))\n",
    "    #print(list)\n",
    "inpTensor2 = torch.stack(list)\n",
    "inpTensor2 = inpTensor2.to(device)\n",
    "print(inpTensor2)\n",
    "print(inpTensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 268])\n"
     ]
    }
   ],
   "source": [
    "test = inpTensor2[:1,:,:]\n",
    "test = test.to(torch.float32)\n",
    "test = test.to(device)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        batch_size = Q.size()[0]\n",
    "        seq_length = Q.size()[1]\n",
    "        #mask = mask.view(batch_size, 1, 1, seq_length).expand(-1, self.num_heads, -1, -1)\n",
    "        mask = mask.unsqueeze(1).unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "        mask = mask.permute(0, 1, 3, 2)\n",
    "        #print(\"mask shape:\",mask.shape)\n",
    "        #print(attn_scores.shape)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == True, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        #print(\"Q:\",Q.shape)\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "\n",
    "        #d_ff: dimension of hidden layer\n",
    "\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding2(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding2, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFromscratch(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(TransformerFromscratch, self).__init__()\n",
    "        #self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        #self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding2(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 10).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 10).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        print(\"seq_length\",seq_length)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        print(\"nopeak:\",nopeak_mask.shape)\n",
    "        print(\"tgt_mask:\",tgt_mask.shape)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def create_pad_mask(self, input : torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "\n",
    "        t = []\n",
    "        for seq in input:\n",
    "            list =[]\n",
    "            for token in seq:\n",
    "                b = False\n",
    "                for value in token:\n",
    "                    if value == pad_token:\n",
    "                        b = True\n",
    "                list.append(b)\n",
    "            t.append(list)\n",
    "        return torch.tensor(t)\n",
    "\n",
    "        #return (input == pad_token)\n",
    "    \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a square matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        #src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_mask = self.create_pad_mask(src,pad_token=10).to(device)\n",
    "        tgt_mask = self.create_pad_mask(tgt,pad_token=10).to(device)\n",
    "        src_embedded = self.dropout(self.positional_encoding(src))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(tgt))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = TransformerFromscratch(tgt_vocab_size=268, d_model=268, num_heads=4, num_layers=8, d_ff=2048, dropout=0.1,max_seq_length=10)\n",
    "model2.to(device)\n",
    "opt = torch.optim.SGD(model2.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[0], batch[1]\n",
    "        #X, y = torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        #sequence_length = y.size(1)\n",
    "        #tgt_mask = model.get_tgt_mask(sequence_length)\n",
    "        \n",
    "        #print(y.view(y.size(0), -1).shape)\n",
    "        #pad_mask = model.create_pad_mask(y, pad_token=-100)\n",
    "        #print(pad_mask)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        \n",
    "        #print(torch.isnan(X).any())\n",
    "        #print(torch.isnan(y).any())\n",
    "\n",
    "        #pred = model(X, y, tgt_mask, pad_mask)\n",
    "\n",
    "        pred = model2(X,y)\n",
    "\n",
    "        #print(\"pred shape:\",pred.shape)\n",
    "        #print(\"y shape:\",y.shape)\n",
    "\n",
    "        y_flattened = y.contiguous().view(-1)\n",
    "\n",
    "        #print(y_flattened)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        #pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        #print(\"prediction:\",pred)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # tryign to print the gradient\n",
    "        #for p in model.parameters():\n",
    "        #    print(p.grad.norm())\n",
    "\n",
    "        #for name, param in model2.named_parameters():\n",
    "        #    if 'weight' in name:\n",
    "         #       print(name)\n",
    "         #       print(param.data.cpu().numpy().shape)\n",
    "         #       print('gradient is \\t', param.grad, '\\trequires grad: ', param.requires_grad)\n",
    "\n",
    "        # gradient clipping to avoid the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_value_(model2.parameters(), 10.)\n",
    "\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train = TensorDataset(inpTensor2.float(), outTensor2.float())\n",
    "batch_size = 200 # Set your desired batch size\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)  # For input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating model\n",
      "------------------------- Epoch 1 -------------------------\n",
      "Training loss: 621.4884\n",
      "\n",
      "------------------------- Epoch 2 -------------------------\n",
      "Training loss: 589.9584\n",
      "\n",
      "------------------------- Epoch 3 -------------------------\n",
      "Training loss: 562.0204\n",
      "\n",
      "------------------------- Epoch 4 -------------------------\n",
      "Training loss: 537.0684\n",
      "\n",
      "------------------------- Epoch 5 -------------------------\n",
      "Training loss: 511.4123\n",
      "\n",
      "------------------------- Epoch 6 -------------------------\n",
      "Training loss: 482.8371\n",
      "\n",
      "------------------------- Epoch 7 -------------------------\n",
      "Training loss: 455.0171\n",
      "\n",
      "------------------------- Epoch 8 -------------------------\n",
      "Training loss: 427.4176\n",
      "\n",
      "------------------------- Epoch 9 -------------------------\n",
      "Training loss: 399.3119\n",
      "\n",
      "------------------------- Epoch 10 -------------------------\n",
      "Training loss: 370.2337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fit(model, opt, loss_fn, train_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model2, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        #validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        #validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        #print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "train_loss_list, validation_loss_list = fit(model2, opt, loss_fn, train_dataloader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 268]) torch.Size([1, 1, 268])\n",
      "tensor([[[ 9.0739,  8.8771,  9.4535, 11.1704,  8.5034, 10.3465,  9.7644,\n",
      "          10.1217, 10.0126, 10.3961,  9.3433, 10.3642,  9.0476, 10.9482,\n",
      "           8.7592,  8.9699,  9.0968, 10.9971, 10.3994,  9.1170,  9.7850,\n",
      "           9.3254,  9.1690,  9.3545,  9.9898,  9.3851,  9.0523,  8.5933,\n",
      "           9.2675,  9.9950,  9.0271,  9.7168,  9.6939,  9.1219,  9.6475,\n",
      "          10.2541,  9.8795,  9.0543,  9.7842,  9.4765,  9.3928,  8.8823,\n",
      "          10.1463,  9.7587, 10.1493,  9.8270, 10.2299,  9.9670,  9.2680,\n",
      "           9.4966,  9.5107,  9.3740,  9.7991,  9.4641,  9.4189,  8.4010,\n",
      "          10.1751, 10.5142, 10.3804,  9.9751,  8.7840, 10.6237,  8.4127,\n",
      "          10.3075,  9.1365,  9.3810,  9.3981,  9.2919,  9.8962,  9.0459,\n",
      "           9.5322,  8.8090,  9.5206, 10.0110,  8.7183,  9.7623,  9.7817,\n",
      "          10.2074,  9.0332,  8.6271,  9.6356,  9.3508,  9.9396, 10.2324,\n",
      "           9.4141,  9.6478,  8.4143,  9.2044,  8.6702,  8.6006,  9.5783,\n",
      "           9.9547,  9.8545, 11.0374, 10.3819,  9.3510,  9.7618,  9.0487,\n",
      "           9.8271, 10.0597,  9.7296,  9.4339,  9.1010,  9.6017, 10.2264,\n",
      "           9.5426,  9.5011,  8.4820,  9.1701,  9.5742,  9.9507,  9.4565,\n",
      "           9.3975,  9.5113, 10.3817,  9.7023,  9.0993,  9.1333, 10.4065,\n",
      "           8.9066,  8.3401,  9.3595,  9.1751,  9.4352,  8.8791,  9.1384,\n",
      "           9.4164, 10.5973,  8.8819, 10.1508, 10.0077,  9.1335,  9.1484,\n",
      "          10.6874,  9.0795, 10.4815,  9.0070,  9.2561,  9.5377,  9.2069,\n",
      "           9.0319,  9.9726,  8.9821,  9.0303, 10.2001,  9.9119,  9.3696,\n",
      "          10.2758,  9.8054, 10.1487, 10.0637, 10.1752,  9.0355,  9.5412,\n",
      "          10.1889, 10.0728, 10.2915,  9.4278, 10.4648,  9.3580,  9.1080,\n",
      "          10.1781,  9.8999,  9.9689,  9.7329,  8.9515,  9.6031,  9.6447,\n",
      "           8.9798,  9.9692,  9.4949, 10.1106,  9.7955,  9.7670,  9.3399,\n",
      "           9.9277,  9.4153, 10.3813, 10.1689,  9.8432,  9.5424, 10.0281,\n",
      "           8.7380,  8.9016,  9.7490,  9.3412,  9.2145,  9.5025, 10.4780,\n",
      "          10.1073,  9.6015,  9.2723,  9.1054,  9.9629,  9.5004,  8.9340,\n",
      "           9.9205,  9.6196,  9.2087,  9.1230,  8.8599,  9.3043,  9.1822,\n",
      "           9.5820,  9.8416,  9.3450,  9.4111,  9.2557,  8.8017,  9.4251,\n",
      "           9.4262,  9.7447,  9.5566, 10.2790,  9.6952, 10.1132, 10.7163,\n",
      "           9.5688, 10.3883,  9.4385,  9.5429, 10.0581, 10.0441,  9.7615,\n",
      "           9.2732,  9.6166, 10.4410, 10.0779,  9.8514, 10.1612, 10.4256,\n",
      "          10.8286,  9.5739, 10.1023,  9.0800, 10.2133,  9.9455,  8.9040,\n",
      "           9.0498, 10.8554,  9.3968,  9.9126,  9.4381,  9.3096,  9.2184,\n",
      "          10.0667,  9.1193, 10.5691,  8.8121,  8.9401,  9.0206,  9.4528,\n",
      "          10.4804,  9.5530, 10.8113,  9.3107,  6.1478,  5.9017,  7.2979,\n",
      "           6.3720,  7.5787,  7.0889,  6.5250,  6.0066,  6.2435,  6.9374,\n",
      "           6.0646,  5.4231]]], grad_fn=<SliceBackward0>) torch.Size([1, 1, 268])\n",
      "tensor([[[30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000, 30.0000,\n",
      "          30.0000, 30.0000],\n",
      "         [ 9.0739,  8.8771,  9.4535, 11.1704,  8.5034, 10.3465,  9.7644,\n",
      "          10.1217, 10.0126, 10.3961,  9.3433, 10.3642,  9.0476, 10.9482,\n",
      "           8.7592,  8.9699,  9.0968, 10.9971, 10.3994,  9.1170,  9.7850,\n",
      "           9.3254,  9.1690,  9.3545,  9.9898,  9.3851,  9.0523,  8.5933,\n",
      "           9.2675,  9.9950,  9.0271,  9.7168,  9.6939,  9.1219,  9.6475,\n",
      "          10.2541,  9.8795,  9.0543,  9.7842,  9.4765,  9.3928,  8.8823,\n",
      "          10.1463,  9.7587, 10.1493,  9.8270, 10.2299,  9.9670,  9.2680,\n",
      "           9.4966,  9.5107,  9.3740,  9.7991,  9.4641,  9.4189,  8.4010,\n",
      "          10.1751, 10.5142, 10.3804,  9.9751,  8.7840, 10.6237,  8.4127,\n",
      "          10.3075,  9.1365,  9.3810,  9.3981,  9.2919,  9.8962,  9.0459,\n",
      "           9.5322,  8.8090,  9.5206, 10.0110,  8.7183,  9.7623,  9.7817,\n",
      "          10.2074,  9.0332,  8.6271,  9.6356,  9.3508,  9.9396, 10.2324,\n",
      "           9.4141,  9.6478,  8.4143,  9.2044,  8.6702,  8.6006,  9.5783,\n",
      "           9.9547,  9.8545, 11.0374, 10.3819,  9.3510,  9.7618,  9.0487,\n",
      "           9.8271, 10.0597,  9.7296,  9.4339,  9.1010,  9.6017, 10.2264,\n",
      "           9.5426,  9.5011,  8.4820,  9.1701,  9.5742,  9.9507,  9.4565,\n",
      "           9.3975,  9.5113, 10.3817,  9.7023,  9.0993,  9.1333, 10.4065,\n",
      "           8.9066,  8.3401,  9.3595,  9.1751,  9.4352,  8.8791,  9.1384,\n",
      "           9.4164, 10.5973,  8.8819, 10.1508, 10.0077,  9.1335,  9.1484,\n",
      "          10.6874,  9.0795, 10.4815,  9.0070,  9.2561,  9.5377,  9.2069,\n",
      "           9.0319,  9.9726,  8.9821,  9.0303, 10.2001,  9.9119,  9.3696,\n",
      "          10.2758,  9.8054, 10.1487, 10.0637, 10.1752,  9.0355,  9.5412,\n",
      "          10.1889, 10.0728, 10.2915,  9.4278, 10.4648,  9.3580,  9.1080,\n",
      "          10.1781,  9.8999,  9.9689,  9.7329,  8.9515,  9.6031,  9.6447,\n",
      "           8.9798,  9.9692,  9.4949, 10.1106,  9.7955,  9.7670,  9.3399,\n",
      "           9.9277,  9.4153, 10.3813, 10.1689,  9.8432,  9.5424, 10.0281,\n",
      "           8.7380,  8.9016,  9.7490,  9.3412,  9.2145,  9.5025, 10.4780,\n",
      "          10.1073,  9.6015,  9.2723,  9.1054,  9.9629,  9.5004,  8.9340,\n",
      "           9.9205,  9.6196,  9.2087,  9.1230,  8.8599,  9.3043,  9.1822,\n",
      "           9.5820,  9.8416,  9.3450,  9.4111,  9.2557,  8.8017,  9.4251,\n",
      "           9.4262,  9.7447,  9.5566, 10.2790,  9.6952, 10.1132, 10.7163,\n",
      "           9.5688, 10.3883,  9.4385,  9.5429, 10.0581, 10.0441,  9.7615,\n",
      "           9.2732,  9.6166, 10.4410, 10.0779,  9.8514, 10.1612, 10.4256,\n",
      "          10.8286,  9.5739, 10.1023,  9.0800, 10.2133,  9.9455,  8.9040,\n",
      "           9.0498, 10.8554,  9.3968,  9.9126,  9.4381,  9.3096,  9.2184,\n",
      "          10.0667,  9.1193, 10.5691,  8.8121,  8.9401,  9.0206,  9.4528,\n",
      "          10.4804,  9.5530, 10.8113,  9.3107,  6.1478,  5.9017,  7.2979,\n",
      "           6.3720,  7.5787,  7.0889,  6.5250,  6.0066,  6.2435,  6.9374,\n",
      "           6.0646,  5.4231]]], grad_fn=<CatBackward0>) torch.Size([1, 2, 268])\n",
      "torch.Size([1, 6, 268]) torch.Size([1, 2, 268])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_input\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 41\u001b[0m svg_animations \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, input_sequence, max_length, SOS_token, EOS_token)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Get source mask\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m#tgt_mask = model.get_tgt_mask(y_input.size(1))\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(input_sequence\u001b[38;5;241m.\u001b[39mshape, y_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 17\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m#print(pred.shape)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#print(pred, pred.shape)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     next_item \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# num with highest probability\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 74\u001b[0m, in \u001b[0;36mTransformerFromscratch.forward\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m     72\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m tgt_embedded\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[1;32m---> 74\u001b[0m     dec_output \u001b[38;5;241m=\u001b[39m \u001b[43mdec_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(dec_output)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, x, enc_output, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, x, x, tgt_mask)\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output))\n\u001b[1;32m---> 15\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output))\n\u001b[0;32m     17\u001b[0m ff_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(x)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\okan2\\anaconda3\\envs\\animationSVG\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[0;32m     41\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_k(K))\n\u001b[0;32m     42\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_v(V))\n\u001b[1;32m---> 44\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_o(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_heads(attn_output))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mMultiHeadAttention.scaled_dot_product_attention\u001b[1;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#print(\"mask shape:\",mask.shape)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#print(attn_scores.shape)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattn_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attn_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_probs, V)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "def predict(model, input_sequence, max_length=6, SOS_token=[[30] * 268], EOS_token=[[50] * 268]):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.float32)\n",
    "\n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        #tgt_mask = model.get_tgt_mask(y_input.size(1))\n",
    "        \n",
    "        print(input_sequence.shape, y_input.shape)\n",
    "        pred = model(input_sequence, y_input)\n",
    "        #print(pred.shape)\n",
    "        #print(pred, pred.shape)\n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "\n",
    "        next_item = torch.tensor([[next_item]])\n",
    "        \n",
    "        next_item = pred[:,:1,:]\n",
    "        print(next_item, next_item.shape)\n",
    "\n",
    "        #print(y_input, next_item)\n",
    "        # Concatenate previous input with predicted best word\n",
    "        #print(y_input.shape, next_item.shape)\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "        print(y_input, y_input.shape)\n",
    "        #print(next_item[0][0][0])\n",
    "        # Stop if model predicts end of sentence\n",
    "        #print(next_item.view(-1).shape)\n",
    "        if next_item[0][0][0] == EOS_token:\n",
    "        #if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()\n",
    "\n",
    "svg_animations = predict(model2, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('AnimateSVG/src')\n",
    "from AnimateSVG.src.pipeline import *\n",
    "\n",
    "for i, row in svg_animations.iterrows():\n",
    "            try:\n",
    "                self._insert_animation(row['animation_id'], row['animation_vector'], filename_suffix=row['model'])\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {row['filename']}\")\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsvg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
