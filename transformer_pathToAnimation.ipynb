{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset,random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir(\"./../.\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x20f574c34c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the output 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\okan2\\AppData\\Local\\Temp\\ipykernel_9584\\2605115553.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_output2['label2'] = filtered_output2['label'].replace(mapping_dict)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>animation_id</th>\n",
       "      <th>model_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, -1.0, -1.0, -1.0, 0.4205715...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>logo_0_animation_0</td>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>21</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>22</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>22</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>logo_99_animation_0</td>\n",
       "      <td>23</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>907 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file  animation_id  \\\n",
       "436   logo_0_animation_0             0   \n",
       "439   logo_0_animation_0             1   \n",
       "438   logo_0_animation_0             3   \n",
       "437   logo_0_animation_0             4   \n",
       "435   logo_0_animation_0             5   \n",
       "..                   ...           ...   \n",
       "892  logo_99_animation_0            21   \n",
       "811  logo_99_animation_0            22   \n",
       "891  logo_99_animation_0            22   \n",
       "810  logo_99_animation_0            23   \n",
       "890  logo_99_animation_0            23   \n",
       "\n",
       "                                          model_output  \n",
       "436  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...  \n",
       "439  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....  \n",
       "438  [0, 0, 0, 0, 0, 1, -1.0, -1.0, -1.0, -1.0, -1....  \n",
       "437  [0, 0, 1, 0, 0, 0, -1.0, -1.0, -1.0, 0.4205715...  \n",
       "435  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....  \n",
       "..                                                 ...  \n",
       "892  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.4...  \n",
       "811  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...  \n",
       "891  [0, 0, 0, 1, 0, 0, -1.0, -1.0, -1.0, -1.0, 0.8...  \n",
       "810  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....  \n",
       "890  [0, 0, 0, 0, 1, 0, -1.0, -1.0, -1.0, -1.0, -1....  \n",
       "\n",
       "[907 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n",
      "tensor(50., dtype=torch.float64)\n",
      "torch.Size([359, 909, 12])\n",
      "tensor([[[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000,  0.8444,  0.7580],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000,  0.8444,  0.7580],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000,  0.8444,  0.7580],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000,  0.8444,  0.7580],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000,  0.8444,  0.7580],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]],\n",
      "\n",
      "        [[30.0000, 30.0000, 30.0000,  ..., 30.0000, 30.0000, 30.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000,  0.8444,  0.7580],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [50.0000, 50.0000, 50.0000,  ..., 50.0000, 50.0000, 50.0000]]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/surrogate_model/animation_label.pkl\", \"rb\") as f:\n",
    "    surrogate2 = pickle.load(f)\n",
    "\n",
    "#Filter with only good or very good ratings\n",
    "#filtered_output = surrogate[surrogate)][['label'].isin(['Good','Very Good'][\"file\",\"animation_id\",\"model_output\",\"label\"]]\n",
    "filtered_output2 = surrogate2[[\"file\",\"animation_id\",\"model_output\",\"label\"]]\n",
    "\n",
    "# dictionary for mapping\n",
    "\n",
    "mapping_dict = {\"Very Good\": 6, \"Good\": 5, \"Bad\": 4,\"Okay\":3, \"Very Bad\": 2, \"no_rating\": 1}\n",
    "\n",
    "# Create another column changing the label into ints\n",
    "\n",
    "filtered_output2['label2'] = filtered_output2['label'].replace(mapping_dict)\n",
    "\n",
    "\n",
    "# get the names of unique logos by splitting with animation number\n",
    "logos = filtered_output2[\"file\"].str.split(\"_animation\").str[0].unique()\n",
    "\n",
    "#print(logos)\n",
    "\n",
    "# create a data frame for the collected best animations\n",
    "bestoutput2 = pd.DataFrame()\n",
    "\n",
    "# go through each logo to find the best animation\n",
    "for logo in logos:\n",
    "\n",
    "    # make a data frame that contains all the animations of one logo\n",
    "    temp = filtered_output2[filtered_output2[\"file\"].str.contains(logo)]\n",
    "\n",
    "    #display(temp)\n",
    "\n",
    "    # create a sum \n",
    "    mean_by_label = temp.groupby('file')['label2'].mean().reset_index()\n",
    "\n",
    "    #print(mean_by_label)\n",
    "\n",
    "    bestlogo = mean_by_label.loc[mean_by_label['label2'].idxmax()]\n",
    "\n",
    "    #print(bestlogo)\n",
    "\n",
    "    # get all the animated paths with the best animation of the logo\n",
    "    best_animations2 = temp[temp[\"file\"]==bestlogo[\"file\"]]\n",
    "\n",
    "    # add to the file\n",
    "    bestoutput2 = pd.concat([bestoutput2,best_animations2],axis=0, ignore_index=True)\n",
    "bestoutput2 = bestoutput2.sort_values(by=['file','animation_id'])[[\"file\", \"animation_id\", \"model_output\"]]\n",
    "display(bestoutput2)\n",
    "\n",
    "\n",
    "\n",
    "filenames = bestoutput2[\"file\"].unique()\n",
    "print(len(filenames))\n",
    "list = []\n",
    "for name in filenames:\n",
    "    seq = bestoutput2[bestoutput2[\"file\"]==name]\n",
    "    seq = seq[\"model_output\"]\n",
    "    seq = pd.DataFrame(bestoutput2[\"model_output\"].to_list())\n",
    "\n",
    "    sos = pd.DataFrame([[30]*12])\n",
    "    \n",
    "    eos = pd.DataFrame([[50]*12])\n",
    "\n",
    "    seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "           \n",
    "    #seq = seq.apply(lambda x: np.array(x).astype(np.float32))\n",
    "    #tokens = []\n",
    "    #for l in seq:\n",
    "    #    tokens.append(torch.tensor(l))\n",
    "\n",
    "    list.append(torch.tensor(seq.values))\n",
    "outTensor2 = torch.stack(list)\n",
    "\n",
    "outTensor2 = outTensor2.to(device)\n",
    "print(outTensor2.max())\n",
    "print(outTensor2.shape)\n",
    "print(outTensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input tensor with the diltered output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data\\embeddings\\path_embedding.pkl\", \"rb\") as f:\n",
    "    inp = pickle.load(f)\n",
    "\n",
    "bestoutput2[\"filename\"] = bestoutput2[\"file\"].str.split(\"_animation\").str[0]\n",
    "\n",
    "display(bestoutput2)\n",
    "\n",
    "inp['animation_id'] = inp['animation_id'].astype(int)\n",
    "\n",
    "#names = bestoutput2[\"file\"].str.replace(\"_animation_0\", \"\")\n",
    "\n",
    "#input = inp[(inp[\"filename\"].isin(bestoutput2[\"filename\"])) & (inp[\"animation_id\"].isin(bestoutput2[\"animation_id\"]))]\n",
    "input = pd.merge(bestoutput2, inp, on=['filename', 'animation_id'], how='inner')\n",
    "input = input.drop(['model_output', 'label', 'label2', 'file'], axis=1)\n",
    "\n",
    "input = input.sort_values(by=['filename','animation_id'])\n",
    "display(input)\n",
    "filenames = input[\"filename\"].unique()\n",
    "#print(filenames)\n",
    "list = []\n",
    "for name in filenames:\n",
    "    #print(name)\n",
    "    seq = input[input[\"filename\"]==name].loc[:, ~inp.columns.isin([\"filename\",\"animation_id\"])][:4]\n",
    "    #print(seq)\n",
    "    seq = pd.concat([seq, pd.DataFrame(0, index=seq.index, columns=range(256, 268))], axis=1, ignore_index=True)\n",
    "\n",
    "    while len(seq) < 4:\n",
    "        seq = pd.concat([seq, pd.DataFrame([[-100]*268])], ignore_index=True)\n",
    "\n",
    "    sos = pd.DataFrame([[30]*268])\n",
    "    \n",
    "    eos = pd.DataFrame([[50]*268])\n",
    "\n",
    "    seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "\n",
    "    list.append(torch.tensor(seq.values))\n",
    "    #print(list)\n",
    "inpTensor2 = torch.stack(list)\n",
    "inpTensor2 = inpTensor2.to(device)\n",
    "print(inpTensor2)\n",
    "print(inpTensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inpTensor2[:1,:,:]\n",
    "test = test.to(torch.float32)\n",
    "test = test.to(device)\n",
    "print(test, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inpTensor2, inpTensor2.shape)\n",
    "\n",
    "print(outTensor2, outTensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None, batch_first=True):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        #src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        #tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        #print(src.shape, tgt.shape)\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    num_tokens=268, dim_model=268, num_heads=4, num_encoder_layers=8, num_decoder_layers=8, dropout_p=0.1\n",
    ").to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[0], batch[1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "\n",
    "        #print(y.shape, y_input, y_input.shape, y_expected, y_expected.shape)\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        #print(X.shape, y.shape)\n",
    "        pred = model(X, y)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        #pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[0], batch[1]\n",
    "            X, y = torch.tensor(X, device=device), torch.tensor(y, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            #print(\"val \", X.shape, y.shape, X.dtype, y.dtype)\n",
    "            pred = model(X, y)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            #pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(TensorDataset(inpTensor2.float(), outTensor2.float()), batch_size=30, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(TensorDataset(inpTensor2.float(), outTensor2.float()),batch_size=30, shuffle=True,  drop_last=True)\n",
    "\n",
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=6, SOS_token=[[30] * 268], EOS_token=[[50] * 268]):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.long, device=device)\n",
    "    \n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
    "        \n",
    "        pred = model(input_sequence, y_input, tgt_mask)\n",
    "        \n",
    "        next_item = pred.topk(1)[1] # num with highest probability\n",
    "        #next_item = torch.tensor([[next_item]], device=device)\n",
    "\n",
    "        next_item = pred[:,:1,:]\n",
    "        #print(next_item, next_item.shape)\n",
    "\n",
    "        #print(input_sequence.shape, y_input.shape, next_item.shape, pred.shape)\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model, test)\n",
    "\n",
    "model_parameters = result[:,1:2,-12:].squeeze().tolist()\n",
    "model_parameters = [value - math.floor(value) for value in model_parameters]\n",
    "model_parameters = [round(value) if index < 6 else value for index, value in enumerate(model_parameters)]\n",
    "print(model_parameters)\n",
    "\n",
    "model_parameters = pd.DataFrame({\"animation_id\" : 2, \"model_output\" : [model_parameters]})\n",
    "\n",
    "print(model_parameters, model_parameters.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 12)\n",
      "                                        model_output  animation_id\n",
      "0  [0, 0, 1, 0, 1, 1, 0.6223583221435547, 0.74209...             1\n",
      "1  [0, 0, 1, 0, 1, 1, 0.6223573684692383, 0.74209...             2\n",
      "2  [0, 0, 1, 0, 1, 1, 0.6223573684692383, 0.74209...             3\n",
      "3  [0, 0, 1, 0, 1, 1, 0.6223573684692383, 0.74209...             4\n",
      "4  [0, 0, 1, 0, 1, 1, 0.6223573684692383, 0.74209...             5\n",
      "5  [0, 0, 1, 0, 1, 1, 0.6223573684692383, 0.74209...             6 (6, 2)\n"
     ]
    }
   ],
   "source": [
    "result = predict(model, test)\n",
    "\n",
    "model_parameters = result[:,1:,-12:].squeeze(0).detach().numpy()\n",
    "print(model_parameters.shape)\n",
    "\n",
    "model_parameters = pd.DataFrame(model_parameters)\n",
    "model_parameters[\"model_output\"] = model_parameters.apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "def process_model_output(lst):\n",
    "    # Floor all values in the list\n",
    "    lst = [value - math.floor(value) for value in lst]\n",
    "    \n",
    "    # Round the first 6 values in the list\n",
    "    lst[:6] = [round(value) for value in lst[:6]]\n",
    "    \n",
    "    return lst\n",
    "\n",
    "# Apply the custom function to the \"model_output\" column\n",
    "model_parameters = model_parameters[['model_output']]\n",
    "model_parameters['model_output'] = model_parameters['model_output'].apply(process_model_output)\n",
    "\n",
    "model_parameters[\"animation_id\"] = range(1, len(model_parameters)+1)\n",
    "print(model_parameters, model_parameters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.postprocessing.postprocessing import *\n",
    "\n",
    "postprocess_logo(model_parameters, \"data/1_inserted_animation_id/logo_0.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animationSVG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
