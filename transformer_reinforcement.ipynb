{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset,random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir(\"./../.\")\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the output 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/surrogate_model/animation_label.pkl\", \"rb\") as f:\n",
    "    surrogate2 = pickle.load(f)\n",
    "\n",
    "#Filter with only good or very good ratings\n",
    "#filtered_output = surrogate[surrogate)][['label'].isin(['Good','Very Good'][\"file\",\"animation_id\",\"model_output\",\"label\"]]\n",
    "filtered_output2 = surrogate2[[\"file\",\"animation_id\",\"model_output\",\"label\"]]\n",
    "\n",
    "# dictionary for mapping\n",
    "\n",
    "mapping_dict = {\"Very Good\": 6, \"Good\": 5, \"Bad\": 4,\"Okay\":3, \"Very Bad\": 2, \"no_rating\": 1}\n",
    "\n",
    "# Create another column changing the label into ints\n",
    "\n",
    "filtered_output2['label2'] = filtered_output2['label'].replace(mapping_dict)\n",
    "\n",
    "\n",
    "# get the names of unique logos by splitting with animation number\n",
    "logos = filtered_output2[\"file\"].str.split(\"_animation\").str[0].unique()\n",
    "\n",
    "#print(logos)\n",
    "\n",
    "# create a data frame for the collected best animations\n",
    "bestoutput2 = pd.DataFrame()\n",
    "\n",
    "# go through each logo to find the best animation\n",
    "for logo in logos:\n",
    "\n",
    "    # make a data frame that contains all the animations of one logo\n",
    "    temp = filtered_output2[filtered_output2[\"file\"].str.contains(logo)]\n",
    "\n",
    "    #display(temp)\n",
    "\n",
    "    # create a sum \n",
    "    mean_by_label = temp.groupby('file')['label2'].mean().reset_index()\n",
    "\n",
    "    #print(mean_by_label)\n",
    "\n",
    "    bestlogo = mean_by_label.loc[mean_by_label['label2'].idxmax()]\n",
    "\n",
    "    #print(bestlogo)\n",
    "\n",
    "    # get all the animated paths with the best animation of the logo\n",
    "    best_animations2 = temp[temp[\"file\"]==bestlogo[\"file\"]]\n",
    "    \n",
    "\n",
    "    # add to the file\n",
    "    bestoutput2 = pd.concat([bestoutput2,best_animations2],axis=0, ignore_index=True)\n",
    "bestoutput2 = bestoutput2.sort_values(by=['file','animation_id'])[[\"file\", \"animation_id\", \"model_output\"]]\n",
    "display(bestoutput2)\n",
    "\n",
    "\n",
    "maxLen = bestoutput2['file'].value_counts().iloc[0]\n",
    "\n",
    "filenames = bestoutput2[\"file\"].unique()\n",
    "print(len(filenames))\n",
    "list = []\n",
    "\n",
    "\n",
    "for name in filenames:\n",
    "    seq = bestoutput2[bestoutput2[\"file\"]==name]\n",
    "\n",
    "    seq = pd.DataFrame(seq[\"model_output\"])\n",
    "    \n",
    "    sos = pd.DataFrame({\"model_output\" : [np.repeat(30, 12)]})\n",
    "    \n",
    "    eos = pd.DataFrame({\"model_output\" : [np.repeat(50, 12)]})\n",
    "    #print(seq.shape, sos.shape, eos.shape)\n",
    "    seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "    seq['model_output'] = seq['model_output'].apply(lambda arr: arr.tolist())\n",
    "    while len(seq) < maxLen+2:\n",
    "        seq = pd.concat([seq, pd.DataFrame({\"model_output\" : [np.repeat(10, 12)]})], ignore_index=True)\n",
    "    \n",
    "  \n",
    "    tensors = []\n",
    "    for value in seq.values:\n",
    "        #print(value, type(value), value.dtype)\n",
    "        tensors.append(torch.tensor(value[0]))\n",
    "    #print(torch.stack(tensors))\n",
    "    list.append(torch.stack(tensors))\n",
    "\n",
    "outTensor2 = torch.stack(list)\n",
    "\n",
    "zeros_to_add = torch.zeros(outTensor2.shape[0], outTensor2.shape[1], 244)\n",
    "outTensor2 = torch.cat([outTensor2, zeros_to_add], dim=2)\n",
    "\n",
    "outTensor2 = outTensor2.to(device)\n",
    "print(outTensor2.max())\n",
    "print(outTensor2.shape)\n",
    "print(outTensor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input tensor with the diltered output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data\\embeddings\\path_embedding.pkl\", \"rb\") as f:\n",
    "    inp = pickle.load(f)\n",
    "\n",
    "bestoutput2[\"filename\"] = bestoutput2[\"file\"].str.split(\"_animation\").str[0]\n",
    "\n",
    "display(bestoutput2)\n",
    "\n",
    "inp['animation_id'] = inp['animation_id'].astype(int)\n",
    "\n",
    "#names = bestoutput2[\"file\"].str.replace(\"_animation_0\", \"\")\n",
    "\n",
    "#input = inp[(inp[\"filename\"].isin(bestoutput2[\"filename\"])) & (inp[\"animation_id\"].isin(bestoutput2[\"animation_id\"]))]\n",
    "input = pd.merge(bestoutput2, inp, on=['filename', 'animation_id'], how='inner')\n",
    "input = input.drop(['model_output', 'file'], axis=1)\n",
    "\n",
    "input = input.sort_values(by=['filename','animation_id'])\n",
    "display(input)\n",
    "maxLen = bestoutput2['file'].value_counts().iloc[0]\n",
    "filenames = input[\"filename\"].unique()\n",
    "#print(filenames)\n",
    "list = []\n",
    "for name in filenames:\n",
    "    #print(name)\n",
    "    seq = input[input[\"filename\"]==name].loc[:, ~inp.columns.isin([\"filename\",\"animation_id\"])]\n",
    "    #print(seq)\n",
    "    #seq = pd.concat([seq, pd.DataFrame(0, index=seq.index, columns=range(256, 268))], axis=1, ignore_index=True)\n",
    "\n",
    "    sos = pd.DataFrame([[30]*256])\n",
    "    \n",
    "    eos = pd.DataFrame([[50]*256])\n",
    "\n",
    "    seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "\n",
    "    while len(seq) < maxLen +2:\n",
    "        seq = pd.concat([seq, pd.DataFrame([[10]*256])], ignore_index=True)\n",
    "\n",
    "    list.append(torch.tensor(seq.values))\n",
    "    #print(list)\n",
    "inpTensor2 = torch.stack(list)\n",
    "inpTensor2 = inpTensor2.to(device)\n",
    "print(inpTensor2)\n",
    "print(inpTensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = inpTensor2[2:3,:,:]\n",
    "test = test.to(torch.float32)\n",
    "test = test.to(device)\n",
    "print(test, test.shape)\n",
    "\n",
    "name = \"logo_104\"\n",
    "seq = input[input[\"filename\"]==name].loc[:, ~inp.columns.isin([\"filename\",\"animation_id\"])]\n",
    "print(seq)\n",
    "sos = pd.DataFrame([[30]*256])\n",
    "    \n",
    "eos = pd.DataFrame([[50]*256])\n",
    "\n",
    "seq = pd.concat([sos, seq, eos], ignore_index=True)\n",
    "\n",
    "while len(seq) < maxLen +2:\n",
    "    seq = pd.concat([seq, pd.DataFrame([[10]*256])], ignore_index=True)\n",
    "\n",
    "test = torch.tensor([seq.values])\n",
    "test = test.to(torch.float32)\n",
    "test = test.to(device)\n",
    "print(test, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inpTensor2, inpTensor2.shape)\n",
    "\n",
    "print(outTensor2, outTensor2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dropout_p,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=5000\n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_tokens, dim_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dropout=dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(dim_model, num_tokens)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None, batch_first=True):\n",
    "        # Src size must be (batch_size, src sequence length)\n",
    "        # Tgt size must be (batch_size, tgt sequence length)\n",
    "\n",
    "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
    "        #src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        #tgt = self.embedding(tgt) * math.sqrt(self.dim_model)\n",
    "        src = self.positional_encoder(src)\n",
    "        tgt = self.positional_encoder(tgt)\n",
    "        \n",
    "\n",
    "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
    "        #print(src.shape, tgt.shape)\n",
    "        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "        out = self.out(transformer_out)\n",
    "        \n",
    "        return out\n",
    "      \n",
    "    def get_tgt_mask(self, size) -> torch.tensor:\n",
    "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
    "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        \n",
    "        # EX for size=5:\n",
    "        # [[0., -inf, -inf, -inf, -inf],\n",
    "        #  [0.,   0., -inf, -inf, -inf],\n",
    "        #  [0.,   0.,   0., -inf, -inf],\n",
    "        #  [0.,   0.,   0.,   0., -inf],\n",
    "        #  [0.,   0.,   0.,   0.,   0.]]\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        mask = []\n",
    "        #print(matrix)\n",
    "        for i in range(0, matrix.size(0)):\n",
    "            seq = []\n",
    "            for j in range(0, matrix.size(1)):\n",
    "                if matrix[i,j,0] == pad_token:\n",
    "                    seq.append(True)\n",
    "                else:\n",
    "                    seq.append(False)\n",
    "            mask.append(seq)\n",
    "        result = torch.tensor(mask)\n",
    "        #print(matrix, result, result.shape)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        typeLoss = nn.CrossEntropyLoss()\n",
    "        ParameterLoss = nn.MSELoss()\n",
    "\n",
    "        loss = 0.5 * typeLoss(inputs[:,:,:6], targets[:,:,:6]) + 0.5 * ParameterLoss(inputs[:,:,6:12], targets[:,:,6:12])\n",
    "        #print(inputs[:,:,:6], inputs[:,:,:6].shape, targets[:,:,6:12], targets[:,:,6:12].shape)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, opt, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        X, y = batch[0], batch[1]\n",
    "        X, y = torch.tensor(X).to(device), torch.tensor(y).to(device)\n",
    "\n",
    "        #print(X.shape, y.shape)\n",
    "        # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "        y_input = y[:,:-1]\n",
    "        y_expected = y[:,1:]\n",
    "\n",
    "        #print(y.shape, y_input, y_input.shape, y_expected, y_expected.shape)\n",
    "        \n",
    "        # Get mask to mask out the next words\n",
    "        sequence_length = y.size(1)\n",
    "        tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "        pad_mask_src = model.create_pad_mask(X, 10).to(device)\n",
    "        pad_mask_tgt = model.create_pad_mask(y, 10).to(device)\n",
    "\n",
    "        # Standard training except we pass in y_input and tgt_mask\n",
    "        #print(X.shape, y.shape)\n",
    "        pred = model(X, y, tgt_mask, src_pad_mask=pad_mask_src, tgt_pad_mask=pad_mask_tgt)\n",
    "\n",
    "        # Permute pred to have batch size first again\n",
    "        #pred = pred.permute(1, 2, 0)      \n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "        total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X, y = batch[0], batch[1]\n",
    "            X, y = torch.tensor(X, device=device), torch.tensor(y, device=device)\n",
    "\n",
    "            # Now we shift the tgt by one so with the <SOS> we predict the token at pos 1\n",
    "            y_input = y[:,:-1]\n",
    "            y_expected = y[:,1:]\n",
    "            \n",
    "            # Get mask to mask out the next words\n",
    "            sequence_length = y.size(1)\n",
    "            tgt_mask = model.get_tgt_mask(sequence_length).to(device)\n",
    "            pad_mask_src = model.create_pad_mask(X, 10).to(device)\n",
    "            pad_mask_tgt = model.create_pad_mask(y, 10).to(device)\n",
    "\n",
    "            # Standard training except we pass in y_input and src_mask\n",
    "            #print(\"val \", X.shape, y.shape, X.dtype, y.dtype)\n",
    "            pred = model(X, y, tgt_mask, src_pad_mask=pad_mask_src, tgt_pad_mask=pad_mask_tgt)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            #pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, y)\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    num_tokens=256, dim_model=256, num_heads=4, num_encoder_layers=8, num_decoder_layers=8, dropout_p=0.1\n",
    ").to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "loss_fn = CustomLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(TensorDataset(inpTensor2.float(), outTensor2.float()), batch_size=100, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(TensorDataset(inpTensor2.float(), outTensor2.float()),batch_size=100, shuffle=True,  drop_last=True)\n",
    "\n",
    "def fit(model, opt, loss_fn, train_dataloader, val_dataloader, epochs):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Used for plotting later on\n",
    "    train_loss_list, validation_loss_list = [], []\n",
    "    \n",
    "    print(\"Training and validating model\")\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
    "        \n",
    "        train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
    "        train_loss_list += [train_loss]\n",
    "        \n",
    "        validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
    "        validation_loss_list += [validation_loss]\n",
    "        \n",
    "        print(f\"Training loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation loss: {validation_loss:.4f}\")\n",
    "        print()\n",
    "        \n",
    "    return train_loss_list, validation_loss_list\n",
    "    \n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "space = {\n",
    "    'lr': hp.loguniform('lr', -5, -2),  # Learning rate in log scale\n",
    "    'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
    "    'num_heads': hp.choice('num_heads', [2, 4, 8]),\n",
    "    'num_encoder_layers': hp.choice('num_encoder_layers', [4, 8, 12]),\n",
    "    'num_decoder_layers': hp.choice('num_decoder_layers', [4, 8, 12]),\n",
    "    'dropout_p': hp.uniform('dropout_p', 0.1, 0.5)\n",
    "}\n",
    "\n",
    "# Objective function to minimize (use your training loop and validation set)\n",
    "def objective(params):\n",
    "    # Replace your existing model with the hyperparameter values\n",
    "    model = Transformer(\n",
    "        num_tokens=256,\n",
    "        dim_model=256,\n",
    "        num_heads=params['num_heads'],\n",
    "        num_encoder_layers=params['num_encoder_layers'],\n",
    "        num_decoder_layers=params['num_decoder_layers'],\n",
    "        dropout_p=params['dropout_p']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Replace your existing optimizer with the hyperparameter values\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=params['lr'])\n",
    "    \n",
    "    # Add more hyperparameters as needed\n",
    "\n",
    "    # Train the model and compute the validation loss\n",
    "    # This is where you insert your training loop and validation set evaluation\n",
    "    try:\n",
    "        train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 10)\n",
    "    \n",
    "\n",
    "        # For hyperopt, return the value to minimize (e.g., negative validation accuracy or positive loss)\n",
    "        return {'loss': validation_loss_list.pop(), 'status': 'ok'}\n",
    "    except:\n",
    "        print(\"fail\")\n",
    "    return 1000000000\n",
    "\n",
    "# Perform hyperparameter search using TPE algorithm\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {\n",
    "    'lr': best['lr'],\n",
    "    'batch_size': [16, 32, 64][best['batch_size']],  # Choose from the original list based on the index\n",
    "    'num_heads': [2, 4, 8][best['num_heads']],\n",
    "    'num_encoder_layers': [4, 8, 12][best['num_encoder_layers']],\n",
    "    'num_decoder_layers': [4, 8, 12][best['num_decoder_layers']],\n",
    "    'dropout_p': best['dropout_p']\n",
    "}\n",
    "\n",
    "# Create the final model with the best hyperparameters\n",
    "model = Transformer(\n",
    "    num_tokens=256,\n",
    "    dim_model=256,\n",
    "    num_heads=best_hyperparameters['num_heads'],\n",
    "    num_encoder_layers=best_hyperparameters['num_encoder_layers'],\n",
    "    num_decoder_layers=best_hyperparameters['num_decoder_layers'],\n",
    "    dropout_p=best_hyperparameters['dropout_p']\n",
    ").to(device)\n",
    "\n",
    "# Create the final optimizer with the best learning rate\n",
    "final_optimizer = torch.optim.SGD(model.parameters(), lr=best_hyperparameters['lr'])\n",
    "\n",
    "# Add more hyperparameters as needed\n",
    "\n",
    "# Train the final model with the best hyperparameters on the entire dataset\n",
    "# This is where you insert your training loop and use the full dataset\n",
    "train_loss_list, validation_loss_list = fit(model, opt, loss_fn, train_dataloader, val_dataloader, 10)\n",
    "\n",
    "# Save the final model if needed\n",
    "#torch.save(model.state_dict(), 'final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "\n",
    "        # Define your action and observation space\n",
    "        self.action_space = spaces.Discrete(2)  # Example: Two discrete actions\n",
    "\n",
    "        # Example: 2-dimensional observation space represented by a Box\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(2, 2), dtype=np.float32)\n",
    "\n",
    "        # Define any other environment parameters\n",
    "        self.max_steps = 100\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to its initial state\n",
    "        self.current_step = 0\n",
    "        # Return the initial observation (2-dimensional tensor)\n",
    "        return np.random.rand(2, 2)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute the given action and return the new state, reward, done flag, and additional info\n",
    "        # For simplicity, let's just return random values\n",
    "        state = np.random.rand(2, 2)\n",
    "        reward = np.random.rand()\n",
    "        done = self.current_step >= self.max_steps\n",
    "        info = {}  # Additional information (optional)\n",
    "        self.current_step += 1\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Render the environment (optional)\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        # Clean up resources (optional)\n",
    "        pass\n",
    "\n",
    "# Example usage:\n",
    "env = CustomEnv()\n",
    "observation = env.reset()\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple transformer model\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.transformer_layer = nn.Transformer(64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.transformer_layer(x)\n",
    "        x = self.fc2(x[-1])  # Using the output of the last transformer layer\n",
    "        return x\n",
    "\n",
    "# Instantiate the custom environment\n",
    "env = CustomEnv()\n",
    "\n",
    "# Set up the transformer model\n",
    "input_size = 2  # Adjust based on your observation space dimensions\n",
    "output_size = 2  # Adjust based on your action space dimensions\n",
    "model = TransformerModel(input_size, output_size)\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        # Forward pass to get action predictions\n",
    "        action_predictions = model(state)\n",
    "\n",
    "        # Sample an action from the output distribution\n",
    "        action = torch.argmax(action_predictions, dim=1).item()\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Calculate the loss (you might need a more sophisticated approach)\n",
    "        target = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        loss = criterion(action_predictions, target)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=31, SOS_token=[[30] * 256], EOS_token=[[50] * 256]):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.long, device=device)\n",
    "    \n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for i in range(1,max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = model.get_tgt_mask(y_input.size(1)).to(device)\n",
    "        \n",
    "        pred = model(input_sequence, y_input, tgt_mask)\n",
    "        \n",
    "        \n",
    "        #todo determine correct path\n",
    "        pred = pred[:,i-1:i,:]\n",
    "\n",
    "        sm = nn.Softmax(dim=2)\n",
    "        next_item = sm(pred[:,:,:6])\n",
    "        #print(next_item)\n",
    "        animation_type = torch.argmax(next_item[:,:,:6], dim=2).item()\n",
    "\n",
    "        for i in range(0,6):\n",
    "            if i == animation_type:\n",
    "                pred[:,:,i] = 1\n",
    "            else:\n",
    "                pred[:,:,i] = 0\n",
    "        \n",
    "        for i in range(12,256):\n",
    "            pred[:,:,i] = 0\n",
    "        \n",
    "        #print(pred)\n",
    "        \n",
    "        #print(input_sequence.shape, y_input.shape, next_item.shape, pred.shape)\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, pred), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item == EOS_token:\n",
    "            print(\"END OF SEQUENCE\")\n",
    "            break\n",
    "\n",
    "    return y_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model, test)\n",
    "print(result, result.shape)\n",
    "#model_parameters = result[:,1:2,-12:].squeeze().tolist()\n",
    "#model_parameters = [value - math.floor(value) for value in model_parameters]\n",
    "#model_parameters = [round(value) if index < 6 else value for index, value in enumerate(model_parameters)]\n",
    "#print(model_parameters)\n",
    "\n",
    "#model_parameters = pd.DataFrame({\"animation_id\" : 2, \"model_output\" : [model_parameters]})\n",
    "\n",
    "#print(model_parameters, model_parameters.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict(model, test)\n",
    "print(result, result.shape)\n",
    "result = result.cpu()\n",
    "model_parameters = result[:,1:,:12].squeeze(0).detach().numpy()\n",
    "print(model_parameters, model_parameters.shape)\n",
    "\n",
    "model_parameters = pd.DataFrame(model_parameters)\n",
    "model_parameters[\"model_output\"] = model_parameters.apply(lambda row: row.tolist(), axis=1)\n",
    "\n",
    "def process_model_output(lst):\n",
    "    # Floor all values in the list\n",
    "    lst = [value - math.floor(value) for value in lst]\n",
    "    \n",
    "    # Round the first 6 values in the list\n",
    "    lst[:6] = [round(value) for value in lst[:6]]\n",
    "    \n",
    "    return lst\n",
    "\n",
    "# Apply the custom function to the \"model_output\" column\n",
    "model_parameters = model_parameters[['model_output']]\n",
    "#model_parameters['model_output'] = model_parameters['model_output'].apply(process_model_output)\n",
    "\n",
    "model_parameters[\"animation_id\"] = range(0, len(model_parameters))\n",
    "print(model_parameters, model_parameters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.postprocessing.postprocessing import *\n",
    "\n",
    "postprocess_logo(model_parameters, \"data/1_inserted_animation_id/logo_104.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animationSVG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
